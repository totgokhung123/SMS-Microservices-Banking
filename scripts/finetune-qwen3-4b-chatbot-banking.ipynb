{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3bd719c5-d39a-47c1-921f-d589de7e238c",
    "_uuid": "56622a65-0c8c-4e5c-bd50-823b1778245a",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "# Qwen/Qwen3-4B Banking Chatbot — Kaggle Notebook\n",
    "\n",
    "Notebook này gom **đúng logic & tham số** từ 4 file của bạn theo thứ tự:\n",
    "1) `data_preprocessing` → 2) `lora_config` → 3) `training_config` → 4) `fine_tune_qwen`.\n",
    "\n",
    "**Lưu ý:**\n",
    "- Mình giữ nguyên code gốc (không đổi logic/hyperparams). Các đường dẫn được cấu hình bằng biến trong notebook để chạy trên Kaggle.\n",
    "- Bạn cần **bật GPU** trong Kaggle (Settings → Accelerator: GPU) trước khi train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1d656d82-cc7e-4030-be3c-3be7a09067c9",
    "_uuid": "b476f23b-0b8d-456c-9ced-3e38530ad429",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-17T17:15:35.754645Z",
     "iopub.status.busy": "2025-09-17T17:15:35.754355Z",
     "iopub.status.idle": "2025-09-17T17:16:59.704379Z",
     "shell.execute_reply": "2025-09-17T17:16:59.703558Z",
     "shell.execute_reply.started": "2025-09-17T17:15:35.754621Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets peft accelerate scikit-learn tokenizers einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T17:16:59.707169Z",
     "iopub.status.busy": "2025-09-17T17:16:59.706912Z",
     "iopub.status.idle": "2025-09-17T17:17:08.288035Z",
     "shell.execute_reply": "2025-09-17T17:17:08.286646Z",
     "shell.execute_reply.started": "2025-09-17T17:16:59.707143Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2d08a541-4f50-4211-8931-2ef218ebdf3e",
    "_uuid": "1bdee4ba-6513-49d7-bea9-cd269fd49a36",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Session 1 — Data Preprocessing\n",
    "Thiết lập đường dẫn Kaggle và chạy pipeline tiền xử lý để xuất `train.jsonl`, `validation.jsonl`, `test.jsonl` và `dataset_stats.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "f8330dd3-a33d-4978-9b71-55507b1417db",
    "_uuid": "7930ca13-1f13-4801-b777-e589e8c65a37",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-17T17:17:08.290127Z",
     "iopub.status.busy": "2025-09-17T17:17:08.289397Z",
     "iopub.status.idle": "2025-09-17T17:17:08.363097Z",
     "shell.execute_reply": "2025-09-17T17:17:08.361956Z",
     "shell.execute_reply.started": "2025-09-17T17:17:08.290076Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Data Preprocessing Script for Qwen2-4B Banking Chatbot Fine-tuning\n",
    "# Xử lý dữ liệu CSV thành format ChatML phù hợp với Qwen2-4B\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import List, Dict, Any\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class BankingDataPreprocessor:\n",
    "    def __init__(self, csv_path: str, output_dir: str):\n",
    "        self.csv_path = csv_path\n",
    "        self.output_dir = output_dir\n",
    "        self.system_prompt = \"\"\"Bạn là trợ lý tư vấn tài chính ngân hàng chuyên nghiệp của HDBank. Bạn có kiến thức sâu về các sản phẩm và dịch vụ ngân hàng, luôn hỗ trợ khách hàng một cách tận tình và chính xác. Hãy trả lời các câu hỏi một cách chi tiết, dễ hiểu và thân thiện.\"\"\"\n",
    "        \n",
    "    def load_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Load CSV data\"\"\"\n",
    "        logger.info(f\"Loading data from {self.csv_path}\")\n",
    "        df = pd.read_csv(self.csv_path, encoding='utf-8')\n",
    "        logger.info(f\"Loaded {len(df)} records\")\n",
    "        return df\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize text\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        # Remove extra whitespaces and normalize\n",
    "        text = str(text).strip()\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        # Remove quotes that might interfere with JSON\n",
    "        text = text.replace('\"\"', '\"')\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def create_chat_format(self, instruction: str, response: str) -> Dict[str, Any]:\n",
    "        \"\"\"Convert instruction-response pair to ChatML format\"\"\"\n",
    "        return {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": self.system_prompt\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": self.clean_text(instruction)\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": self.clean_text(response)\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def process_data(self, df: pd.DataFrame) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Process DataFrame to ChatML format\"\"\"\n",
    "        processed_data = []\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            if pd.isna(row['instruction']) or pd.isna(row['response']):\n",
    "                logger.warning(f\"Skipping row {idx} due to missing data\")\n",
    "                continue\n",
    "                \n",
    "            chat_data = self.create_chat_format(\n",
    "                instruction=row['instruction'],\n",
    "                response=row['response']\n",
    "            )\n",
    "            processed_data.append(chat_data)\n",
    "            \n",
    "        logger.info(f\"Processed {len(processed_data)} valid records\")\n",
    "        return processed_data\n",
    "    \n",
    "    def split_data(self, data: List[Dict[str, Any]], \n",
    "                   train_ratio: float = 0.8, \n",
    "                   val_ratio: float = 0.1,\n",
    "                   test_ratio: float = 0.1) -> tuple:\n",
    "        \"\"\"Split data into train/validation/test sets\"\"\"\n",
    "        assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, \"Ratios must sum to 1.0\"\n",
    "        \n",
    "        # First split: train + val vs test\n",
    "        train_val, test = train_test_split(\n",
    "            data, \n",
    "            test_size=test_ratio, \n",
    "            random_state=42,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        # Second split: train vs val\n",
    "        val_size = val_ratio / (train_ratio + val_ratio)\n",
    "        train, val = train_test_split(\n",
    "            train_val,\n",
    "            test_size=val_size,\n",
    "            random_state=42,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Data split - Train: {len(train)}, Val: {len(val)}, Test: {len(test)}\")\n",
    "        return train, val, test\n",
    "    \n",
    "    def save_jsonl(self, data: List[Dict[str, Any]], filename: str):\n",
    "        \"\"\"Save data in JSONL format\"\"\"\n",
    "        filepath = os.path.join(self.output_dir, filename)\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        \n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            for item in data:\n",
    "                f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "        logger.info(f\"Saved {len(data)} records to {filepath}\")\n",
    "    \n",
    "    def generate_statistics(self, data: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"Generate dataset statistics\"\"\"\n",
    "        total_samples = len(data)\n",
    "        \n",
    "        # Calculate text lengths\n",
    "        instruction_lengths = []\n",
    "        response_lengths = []\n",
    "        \n",
    "        for item in data:\n",
    "            messages = item['messages']\n",
    "            user_msg = next(msg for msg in messages if msg['role'] == 'user')\n",
    "            assistant_msg = next(msg for msg in messages if msg['role'] == 'assistant')\n",
    "            \n",
    "            instruction_lengths.append(len(user_msg['content']))\n",
    "            response_lengths.append(len(assistant_msg['content']))\n",
    "        \n",
    "        stats = {\n",
    "            'total_samples': total_samples,\n",
    "            'avg_instruction_length': sum(instruction_lengths) / len(instruction_lengths),\n",
    "            'avg_response_length': sum(response_lengths) / len(response_lengths),\n",
    "            'max_instruction_length': max(instruction_lengths),\n",
    "            'max_response_length': max(response_lengths),\n",
    "            'min_instruction_length': min(instruction_lengths),\n",
    "            'min_response_length': min(response_lengths)\n",
    "        }\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def run_preprocessing(self):\n",
    "        \"\"\"Main preprocessing pipeline\"\"\"\n",
    "        logger.info(\"Starting data preprocessing...\")\n",
    "        \n",
    "        # Load and process data\n",
    "        df = self.load_data()\n",
    "        processed_data = self.process_data(df)\n",
    "        \n",
    "        # Split data\n",
    "        train_data, val_data, test_data = self.split_data(processed_data)\n",
    "        \n",
    "        # Save datasets\n",
    "        self.save_jsonl(train_data, 'train.jsonl')\n",
    "        self.save_jsonl(val_data, 'validation.jsonl')\n",
    "        self.save_jsonl(test_data, 'test.jsonl')\n",
    "        \n",
    "        # Generate and save statistics\n",
    "        stats = self.generate_statistics(processed_data)\n",
    "        stats_path = os.path.join(self.output_dir, 'dataset_stats.json')\n",
    "        with open(stats_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(stats, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        logger.info(\"Preprocessing completed successfully!\")\n",
    "        logger.info(f\"Dataset statistics: {stats}\")\n",
    "        \n",
    "        return train_data, val_data, test_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## **Cấu hình đường dẫn & chạy tiền xử lý**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T17:17:08.364649Z",
     "iopub.status.busy": "2025-09-17T17:17:08.364304Z",
     "iopub.status.idle": "2025-09-17T17:17:13.370423Z",
     "shell.execute_reply": "2025-09-17T17:17:13.369731Z",
     "shell.execute_reply.started": "2025-09-17T17:17:08.364617Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Số mẫu Train/Val/Test: 20434 2555 2555\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# 👉 SỬA đường dẫn này theo dataset bạn đã gắn vào notebook\n",
    "# Ví dụ nếu dataset name là: hdbank-finetune-data, chứa file final_sua_mapped_v2.csv\n",
    "CSV_PATH = \"/kaggle/input/data-banking-processed/final_sua_mapped_v2.csv\"\n",
    "\n",
    "# Thư mục output nên để trong /kaggle/working\n",
    "OUTPUT_DIR = \"/kaggle/working/data/processed/train_split\"\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "preprocessor = BankingDataPreprocessor(CSV_PATH, OUTPUT_DIR)\n",
    "train_data, val_data, test_data = preprocessor.run_preprocessing()\n",
    "\n",
    "print(\"Số mẫu Train/Val/Test:\", len(train_data), len(val_data), len(test_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## **Xem nhanh dữ liệu & thống kê**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T17:17:13.371614Z",
     "iopub.status.busy": "2025-09-17T17:17:13.371299Z",
     "iopub.status.idle": "2025-09-17T17:17:13.380731Z",
     "shell.execute_reply": "2025-09-17T17:17:13.379939Z",
     "shell.execute_reply.started": "2025-09-17T17:17:13.371586Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> /kaggle/working/data/processed/train_split/train.jsonl\n",
      "{'messages': [{'role': 'system', 'content': 'Bạn là trợ lý tư vấn tài chính ngân hàng chuyên nghiệp của HDBank. Bạn có kiến thức sâu về các sản phẩm và dịch vụ ngân hàng, luôn hỗ trợ khách hàng một cách tận tình và chính xác. Hãy trả lời các câu hỏi một cách chi tiết, dễ hiểu và thân thiện.'}, {'role': 'user', 'content': 'I cant find my master card will uhlep me lock it (tạm dịch: Tôi sẽ tìm thấy thẻ chủ của tôi sẽ uhlep tôi khóa nó)'}, {'role': 'assistant', 'content': 'Xin vui lòng chấp nhận lời xin lỗi của tôi cho bất kỳ khó chịu gây ra bởi mất thẻ Master của bạn. Tôi ở đây để giúp bạn đóng nó cho sự bình an của bạn. Để giúp bạn với điều này, xin vui lòng làm theo các bước sau đây: 1. Kiểm tra ví của bạn, túi, hoặc bất kỳ địa điểm gần đây bạn đã truy cập để xem nếu bạn có thể tìm thấy thẻ Master của bạn. Nếu bạn tìm thấy nó, đảm bảo bạn giữ nó an toàn. 2. Nếu bạn không thể tìm thấy thẻ của bạn, điều quan trọng là để báo cáo mất ngay lập tức. Liên hệ với nhóm hỗ trợ khách hàng của chúng tôi tại {{SUPPORT_PHONE}} hoặc truy cập trang web của chúng tôi tại {{BILL_PAY_URL}} để báo cáo thẻ mất. 3. đại diện hỗ trợ khách hàng của chúng tôi sẽ hướng dẫn bạn thông qua các bước cần thiết để đóng thẻ Master của bạn và đảm bảo an toàn. Họ cũng sẽ giúp bạn với bất kỳ hướng dẫn nào khác hoặc bạn có thể nhớ.'}]}\n",
      "{'messages': [{'role': 'system', 'content': 'Bạn là trợ lý tư vấn tài chính ngân hàng chuyên nghiệp của HDBank. Bạn có kiến thức sâu về các sản phẩm và dịch vụ ngân hàng, luôn hỗ trợ khách hàng một cách tận tình và chính xác. Hãy trả lời các câu hỏi một cách chi tiết, dễ hiểu và thân thiện.'}, {'role': 'user', 'content': 'Tôi muốn giúp đỡ để tranh luận về một giao dịch ATM'}, {'role': 'assistant', 'content': 'Tôi xin lỗi khi nghe rằng bạn đang gặp một vấn đề với tranh chấp một giao dịch ATM. Tôi ở đây để giúp bạn ra. Dưới đây là các bước bạn có thể thực hiện để tranh chấp giao dịch: 1. Liên hệ với dịch vụ khách hàng của ngân hàng của bạn: Hãy giải thích vấn đề: Đặt cho họ một cuộc gọi tại số được cung cấp trên mặt sau của thẻ debit / tín dụng của bạn hoặc trên trang web của ngân hàng của bạn. Họ sẽ hướng dẫn bạn thông qua quá trình tranh chấp giao dịch. 2. Cung cấp chi tiết giao dịch: Hãy chắc chắn rằng bạn có thông tin cần thiết về giao dịch, chẳng hạn như ngày, thời gian, vị trí và số tiền. Điều này sẽ giúp đại diện dịch vụ khách hàng giúp bạn hiệu quả hơn. 3. Giải thích vấn đề: Bạn có thể giải thích rõ lý do tại sao bạn tranh chấp giao dịch ATM. Nó có thể là do một giao dịch không được ủy quyền, một sự phân biệt về số tiền, hoặc bất kỳ chi tiết khác liên quan để giải quyết vấn. họ có thể giúp họ trong quá trình giải quyết giao dịch. 5. nếu bạn có thể giúp đỡ ngân hàng để giải quyết vấn đề.'}]}\n",
      "==> /kaggle/working/data/processed/train_split/validation.jsonl\n",
      "{'messages': [{'role': 'system', 'content': 'Bạn là trợ lý tư vấn tài chính ngân hàng chuyên nghiệp của HDBank. Bạn có kiến thức sâu về các sản phẩm và dịch vụ ngân hàng, luôn hỗ trợ khách hàng một cách tận tình và chính xác. Hãy trả lời các câu hỏi một cách chi tiết, dễ hiểu và thân thiện.'}, {'role': 'user', 'content': 'Tôi muốn kiểm tra thanh toán tiền ủng hộ, bạn có thể giúp tôi không?'}, {'role': 'assistant', 'content': 'Tôi ở đây để giúp bạn kiểm tra thanh toán tiền ảo. Chúng tôi bắt đầu! Để kiểm tra thanh toán tiền ảo của bạn, bạn có một vài tùy chọn: 1. Tài khoản trực tuyến: Nếu bạn có một tài khoản trực tuyến với chúng tôi, bạn có thể đăng nhập và duyệt đến phần \" tiền ảo\". Ở đó, bạn sẽ có thể xem chi tiết thanh toán tiền ảo của bạn, bao gồm số tiền nợ, ngày nợ, và bất kỳ cân bằng xuất sắc. 2. Hỗ trợ khách hàng: Nếu bạn thích hỗ trợ cá nhân, đội hỗ trợ khách hàng chuyên dụng của chúng tôi có sẵn để giúp bạn. Bạn có thể truy cập chúng tại {{SUPPORT_PHONE}} trong {{WORKING_HOURS}}. Họ sẽ hơn hạnh phúc cung cấp cho bạn thông tin bạn cần và trả lời bất kỳ câu hỏi bạn có thể có. Nếu bạn có bất kỳ câu hỏi hoặc cần thêm hỗ trợ, hãy cảm thấy miễn phí để biết tôi.'}]}\n",
      "{'messages': [{'role': 'system', 'content': 'Bạn là trợ lý tư vấn tài chính ngân hàng chuyên nghiệp của HDBank. Bạn có kiến thức sâu về các sản phẩm và dịch vụ ngân hàng, luôn hỗ trợ khách hàng một cách tận tình và chính xác. Hãy trả lời các câu hỏi một cách chi tiết, dễ hiểu và thân thiện.'}, {'role': 'user', 'content': 'Tôi muốn kích hoạt một thẻ tín dụng trực tuyến nơi có thể ido nó'}, {'role': 'assistant', 'content': 'Chắc chắn! tôi ở đây để giúp bạn kích hoạt thẻ tín dụng của bạn trực tuyến. để kích hoạt thẻ tín dụng của bạn, vui lòng làm theo các bước sau đây: 1. truy cập trang web chính thức của nhà cung cấp thẻ tín dụng của bạn. 2. tìm kiếm tất cả các chi tiết cần thiết, bấm vào phần \"Activate Card\" hoặc \"Submit\" nút. Nó thường nằm trong \"Account\" hoặc \"Online Services\" khu vực của trang web. 3. bấm vào tùy chọn \"Activate Card\" 4. bạn có thể được yêu cầu cung cấp một số thông tin như số thẻ của bạn, tên, ngày hết hạn, và mã bảo mật. Hãy chắc chắn rằng thẻ của bạn có sẵn. 5. cẩn thận nhập thông tin cần thiết và làm theo bất kỳ hướng dẫn bổ sung được cung cấp trên trang web. 6. Sau khi bạn đã nhập tất cả các chi tiết cần thiết, bấm vào \"Activate Card\" hoặc \"Submit\" nút. 7. Sau khi kích hoạt thẻ của bạn, bạn có thể được khuyến khích để tạo một tài khoản trực tuyến cho số thẻ tín dụng hoặc thẻ tín dụng của bạn.'}]}\n",
      "==> /kaggle/working/data/processed/train_split/test.jsonl\n",
      "{'messages': [{'role': 'system', 'content': 'Bạn là trợ lý tư vấn tài chính ngân hàng chuyên nghiệp của HDBank. Bạn có kiến thức sâu về các sản phẩm và dịch vụ ngân hàng, luôn hỗ trợ khách hàng một cách tận tình và chính xác. Hãy trả lời các câu hỏi một cách chi tiết, dễ hiểu và thân thiện.'}, {'role': 'user', 'content': 'Tôi muốn hủy khoản vay, giúp tôi'}, {'role': 'assistant', 'content': 'Tôi xin lỗi khi nghe rằng bạn đang tìm cách hủy khoản vay của bạn. tôi hiểu rằng hoàn cảnh có thể thay đổi, vì vậy tôi sẽ làm tốt nhất để giúp bạn với quá trình hủy khoản vay. Để hủy khoản vay của bạn, vui lòng làm theo các bước sau đây: 1. Liên hệ với nhóm dịch vụ khách hàng của chúng tôi tại {{SUPPORT_PHONE}} hoặc gửi email đến {{SUPPORT_EMAIL}} với yêu cầu của bạn. họ sẽ hướng dẫn bạn thông qua quá trình hủy khoản vay và cung cấp bất kỳ hình thức hoặc tài liệu cần thiết. 2. Cung cấp chi tiết khoản vay của bạn, chẳng hạn như số khoản vay, số khoản vay, và bất kỳ thông tin liên quan khác, cho đại diện dịch vụ khách hàng. Điều này sẽ giúp tăng tốc quá trình hủy khoản vay. 3. Nếu có bất kỳ cân bằng hoặc phí liên quan đến khoản vay của bạn, hãy chắc chắn để thảo luận với đại diện dịch vụ khách hàng.'}]}\n",
      "{'messages': [{'role': 'system', 'content': 'Bạn là trợ lý tư vấn tài chính ngân hàng chuyên nghiệp của HDBank. Bạn có kiến thức sâu về các sản phẩm và dịch vụ ngân hàng, luôn hỗ trợ khách hàng một cách tận tình và chính xác. Hãy trả lời các câu hỏi một cách chi tiết, dễ hiểu và thân thiện.'}, {'role': 'user', 'content': 'Tôi phải chuyển, tôi cần sự giúp đỡ'}, {'role': 'assistant', 'content': 'Tôi ở đây để giúp bạn thực hiện một chuyển khoản. Để đảm bảo một quá trình thuận tiện, vui lòng làm theo các bước sau đây: 1. Đăng nhập vào tài khoản ngân hàng trực tuyến hoặc ứng dụng ngân hàng di động của bạn. 2. Truy cập vào phần \"Tài khoản\" hoặc \" tiền\" phần. 3. Chọn tùy chọn để thực hiện một chuyển khoản mới đến tài khoản ngân hàng khác. 4. Cung cấp cho tài khoản ngân hàng của người nhận thông tin, chẳng hạn như số tài khoản và số định tuyến. Đơn giản kiểm tra thông tin để xác định chính xác. 5. Chọn số khoản chuyển khoản và chọn tài khoản từ đó bạn muốn gửi tiền. 6. Nếu được thúc đẩy, thêm một tham chiếu hoặc ghi chú cho chuyển khoản. 7. Xem chi tiết kỹ lưỡng và đảm bảo mọi thứ là chính xác. 8. Cuối cùng, xác nhận chuyển khoản để khởi động. Nếu bạn gặp bất kỳ khó khăn hoặc có câu hỏi cụ thể trong quá trình, cảm thấy miễn phí để truy cập vào số điện thoại.'}]}\n",
      "\n",
      "==> dataset_stats.json\n",
      "{'total_samples': 25544, 'avg_instruction_length': 60.644808957093645, 'avg_response_length': 751.8644691512684, 'max_instruction_length': 2044, 'max_response_length': 3481, 'min_instruction_length': 7, 'min_response_length': 76}\n"
     ]
    }
   ],
   "source": [
    "import json, os, itertools\n",
    "\n",
    "def peek_jsonl(path, n=3):\n",
    "    print(f\"==> {path}\")\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in zip(range(n), f):\n",
    "            print(json.loads(line))\n",
    "\n",
    "base = OUTPUT_DIR\n",
    "for name in [\"train.jsonl\", \"validation.jsonl\", \"test.jsonl\"]:\n",
    "    peek_jsonl(os.path.join(base, name), n=2)\n",
    "\n",
    "print(\"\\n==> dataset_stats.json\")\n",
    "with open(os.path.join(base, \"dataset_stats.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "    print(json.load(f))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4c94a421-d46b-471c-b670-06549cd80cb7",
    "_uuid": "aeea72f3-9375-4523-9262-ec07341a82ae",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Session 2 — LoRA Configuration\n",
    "Import nguyên `lora_config.py` và in tóm tắt cấu hình + ước lượng bộ nhớ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "b0f1853e-7957-4219-8878-98e3a98f0a0e",
    "_uuid": "60c4df52-5258-42c3-915e-0385b6466ae2",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-17T17:17:13.382180Z",
     "iopub.status.busy": "2025-09-17T17:17:13.381868Z",
     "iopub.status.idle": "2025-09-17T17:17:13.404496Z",
     "shell.execute_reply": "2025-09-17T17:17:13.403586Z",
     "shell.execute_reply.started": "2025-09-17T17:17:13.382150Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LoRA Configuration for Qwen3-4B Banking Chatbot Fine-tuning\n",
    "Cấu hình LoRA được tối ưu cho VRAM và chất lượng training\n",
    "\"\"\"\n",
    "\n",
    "from peft import LoraConfig, TaskType\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class LoRAConfigManager:\n",
    "    \"\"\"\n",
    "    LoRA Configuration Manager với các tham số được tối ưu cho Qwen3-4B\n",
    "    \n",
    "    Phân tích tham số:\n",
    "    - r=16: Rank thấp vừa đủ để capture patterns quan trọng, tiết kiệm VRAM\n",
    "    - lora_alpha=32: Scaling factor = 2*r, cân bằng tốt giữa stability và learning capacity\n",
    "    - target_modules: Tập trung vào attention và MLP layers quan trọng nhất\n",
    "    - lora_dropout=0.1: Regularization vừa phải, tránh overfitting\n",
    "    \"\"\"\n",
    "    \n",
    "    # Core LoRA parameters\n",
    "    r: int = 16                    # Rank - cân bằng hiệu suất/chất lượng\n",
    "    lora_alpha: int = 32           # Scaling factor (2*r)\n",
    "    lora_dropout: float = 0.1      # Dropout cho LoRA layers\n",
    "    bias: str = \"none\"             # Không train bias để tiết kiệm memory\n",
    "    \n",
    "    # Target modules cho Qwen3-4B architecture\n",
    "    target_modules: List[str] = None\n",
    "    \n",
    "    # Task configuration\n",
    "    task_type: TaskType = TaskType.CAUSAL_LM\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Initialize target modules if not provided\"\"\"\n",
    "        if self.target_modules is None:\n",
    "            self.target_modules = [\n",
    "                # Attention layers - quan trọng nhất cho language understanding\n",
    "                \"q_proj\",      # Query projection\n",
    "                \"k_proj\",      # Key projection  \n",
    "                \"v_proj\",      # Value projection\n",
    "                \"o_proj\",      # Output projection\n",
    "                \n",
    "                # MLP layers - quan trọng cho knowledge representation\n",
    "                \"gate_proj\",   # Gate projection in MLP\n",
    "                \"up_proj\",     # Up projection in MLP\n",
    "                \"down_proj\",   # Down projection in MLP\n",
    "            ]\n",
    "    \n",
    "    def get_lora_config(self) -> LoraConfig:\n",
    "        \"\"\"Create LoRA configuration\"\"\"\n",
    "        return LoraConfig(\n",
    "            r=self.r,\n",
    "            lora_alpha=self.lora_alpha,\n",
    "            target_modules=self.target_modules,\n",
    "            lora_dropout=self.lora_dropout,\n",
    "            bias=self.bias,\n",
    "            task_type=self.task_type,\n",
    "            inference_mode=False,\n",
    "        )\n",
    "    \n",
    "    def get_memory_efficient_config(self) -> LoraConfig:\n",
    "        \"\"\"Get memory-efficient LoRA config for limited VRAM\"\"\"\n",
    "        return LoraConfig(\n",
    "            r=8,                    # Giảm rank để tiết kiệm memory\n",
    "            lora_alpha=16,          # Tương ứng giảm alpha\n",
    "            target_modules=[\n",
    "                \"q_proj\", \"v_proj\",  # Chỉ train query và value\n",
    "                \"down_proj\"          # Và output MLP\n",
    "            ],\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\",\n",
    "            task_type=self.task_type,\n",
    "            inference_mode=False,\n",
    "        )\n",
    "    \n",
    "    def get_high_quality_config(self) -> LoraConfig:\n",
    "        \"\"\"Get high-quality LoRA config for better performance\"\"\"\n",
    "        return LoraConfig(\n",
    "            r=32,                   # Rank cao hơn cho chất lượng tốt hơn\n",
    "            lora_alpha=64,          # Alpha tương ứng\n",
    "            target_modules=self.target_modules + [\n",
    "                \"embed_tokens\",     # Thêm embedding layer\n",
    "                \"lm_head\"          # Thêm language model head\n",
    "            ],\n",
    "            lora_dropout=0.05,      # Dropout thấp hơn\n",
    "            bias=\"none\",\n",
    "            task_type=self.task_type,\n",
    "            inference_mode=False,\n",
    "        )\n",
    "    \n",
    "    def estimate_memory_usage(self, model_size_gb: float = 8.0) -> dict:\n",
    "        \"\"\"Estimate memory usage for different configurations\"\"\"\n",
    "        \n",
    "        # Base model memory\n",
    "        base_memory = model_size_gb\n",
    "        \n",
    "        # LoRA parameters estimation\n",
    "        # Rough calculation: r * (input_dim + output_dim) * num_layers * num_target_modules\n",
    "        qwen_hidden_size = 3584  # Qwen3-4B hidden size\n",
    "        num_layers = 40          # Qwen3-4B layers\n",
    "        num_target_modules = len(self.target_modules)\n",
    "        \n",
    "        # Standard config\n",
    "        standard_params = self.r * qwen_hidden_size * num_layers * num_target_modules\n",
    "        standard_memory = standard_params * 4 / (1024**3)  # 4 bytes per param, convert to GB\n",
    "        \n",
    "        # Memory efficient config  \n",
    "        efficient_params = 8 * qwen_hidden_size * num_layers * 3  # r=8, 3 modules\n",
    "        efficient_memory = efficient_params * 4 / (1024**3)\n",
    "        \n",
    "        # High quality config\n",
    "        quality_params = 32 * qwen_hidden_size * num_layers * (num_target_modules + 2)\n",
    "        quality_memory = quality_params * 4 / (1024**3)\n",
    "        \n",
    "        return {\n",
    "            \"base_model_memory_gb\": base_memory,\n",
    "            \"standard_lora_memory_gb\": standard_memory,\n",
    "            \"efficient_lora_memory_gb\": efficient_memory,\n",
    "            \"quality_lora_memory_gb\": quality_memory,\n",
    "            \"total_standard_gb\": base_memory + standard_memory,\n",
    "            \"total_efficient_gb\": base_memory + efficient_memory,\n",
    "            \"total_quality_gb\": base_memory + quality_memory\n",
    "        }\n",
    "    \n",
    "    def print_config_summary(self):\n",
    "        \"\"\"Print configuration summary\"\"\"\n",
    "        print(\"=== LoRA Configuration Summary ===\")\n",
    "        print(f\"Rank (r): {self.r}\")\n",
    "        print(f\"Alpha: {self.lora_alpha}\")\n",
    "        print(f\"Dropout: {self.lora_dropout}\")\n",
    "        print(f\"Target modules: {len(self.target_modules)}\")\n",
    "        print(f\"Modules: {', '.join(self.target_modules)}\")\n",
    "        \n",
    "        memory_info = self.estimate_memory_usage()\n",
    "        print(f\"\\n=== Memory Estimation ===\")\n",
    "        print(f\"Standard config total: {memory_info['total_standard_gb']:.2f} GB\")\n",
    "        print(f\"Efficient config total: {memory_info['total_efficient_gb']:.2f} GB\") \n",
    "        print(f\"Quality config total: {memory_info['total_quality_gb']:.2f} GB\")\n",
    "\n",
    "def get_recommended_config(available_vram_gb: float) -> LoraConfig:\n",
    "    \"\"\"Get recommended LoRA config based on available VRAM\"\"\"\n",
    "    config_manager = LoRAConfigManager()\n",
    "    \n",
    "    if available_vram_gb >= 16:\n",
    "        print(\"Using high-quality LoRA configuration\")\n",
    "        return config_manager.get_high_quality_config()\n",
    "    elif available_vram_gb >= 12:\n",
    "        print(\"Using standard LoRA configuration\")\n",
    "        return config_manager.get_lora_config()\n",
    "    else:\n",
    "        print(\"Using memory-efficient LoRA configuration\")\n",
    "        return config_manager.get_memory_efficient_config()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5fcf2590-3de3-4862-a2fb-e3010a17262f",
    "_uuid": "78f0ebcd-8036-41e7-83ec-309c4a75db2a",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Session 3 — Training Configuration\n",
    "Import nguyên `training_config.py`, in tóm tắt cấu hình và lịch train ước tính dựa trên số mẫu đã tiền xử lý."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "c4e1f5ca-361b-4d3b-903a-3e034ea4f6b1",
    "_uuid": "4720a8a7-dd31-40c2-b00d-7564e72a6156",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-17T17:17:13.410913Z",
     "iopub.status.busy": "2025-09-17T17:17:13.410194Z",
     "iopub.status.idle": "2025-09-17T17:17:13.489469Z",
     "shell.execute_reply": "2025-09-17T17:17:13.488592Z",
     "shell.execute_reply.started": "2025-09-17T17:17:13.410887Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Training Configuration for Qwen2.5-4B Banking Chatbot Fine-tuning\n",
    "Cấu hình training được tối ưu cho chất lượng và hiệu suất\n",
    "\"\"\"\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "import torch\n",
    "import os\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfigManager:\n",
    "    \"\"\"\n",
    "    Training Configuration Manager với các tham số được nghiên cứu kỹ lưỡng\n",
    "    \n",
    "    Phân tích tham số:\n",
    "    - learning_rate=2e-4: Optimal cho LoRA fine-tuning, không quá cao gây instability\n",
    "    - batch_size=4: Cân bằng giữa memory usage và gradient stability\n",
    "    - gradient_accumulation=4: Effective batch size = 16, đủ lớn cho stable training\n",
    "    - warmup_steps=100: Warm-up để tránh gradient explosion ở đầu training\n",
    "    - max_steps: Được tính dựa trên dataset size và epochs\n",
    "    \"\"\"\n",
    "    \n",
    "    # Core training parameters\n",
    "    output_dir: str = \"./qwen-banking-lora\"\n",
    "    num_train_epochs: int = 3\n",
    "    per_device_train_batch_size: int = 4\n",
    "    per_device_eval_batch_size: int = 4\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    \n",
    "    # Learning rate and optimization\n",
    "    learning_rate: float = 2e-4\n",
    "    weight_decay: float = 0.01\n",
    "    warmup_steps: int = 100\n",
    "    lr_scheduler_type: str = \"cosine\"\n",
    "    \n",
    "    # Mixed precision and optimization\n",
    "    fp16: bool = True\n",
    "    bf16: bool = False  # Set to True if using Ampere+ GPUs\n",
    "    gradient_checkpointing: bool = True\n",
    "    dataloader_pin_memory: bool = False\n",
    "    \n",
    "    # Evaluation and logging\n",
    "    eval_strategy: str = \"steps\"\n",
    "    eval_steps: int = 100\n",
    "    logging_steps: int = 10\n",
    "    save_strategy: str = \"steps\"\n",
    "    save_steps: int = 200\n",
    "    save_total_limit: int = 3\n",
    "    \n",
    "    # Early stopping and best model\n",
    "    load_best_model_at_end: bool = True\n",
    "    metric_for_best_model: str = \"eval_loss\"\n",
    "    greater_is_better: bool = False\n",
    "    \n",
    "    # Memory optimization\n",
    "    remove_unused_columns: bool = False\n",
    "    dataloader_num_workers: int = 0\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Post-initialization setup\"\"\"\n",
    "        # Auto-detect bf16 support\n",
    "        if torch.cuda.is_available():\n",
    "            device_capability = torch.cuda.get_device_capability()\n",
    "            if device_capability[0] >= 8:  # Ampere or newer\n",
    "                self.bf16 = True\n",
    "                self.fp16 = False\n",
    "                print(\"Using BF16 precision (Ampere+ GPU detected)\")\n",
    "            else:\n",
    "                print(\"Using FP16 precision\")\n",
    "        \n",
    "        # Create output directory\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "    \n",
    "    def get_training_args(self, max_steps: Optional[int] = None) -> TrainingArguments:\n",
    "        \"\"\"Create TrainingArguments object\"\"\"\n",
    "        \n",
    "        args_dict = {\n",
    "            \"output_dir\": self.output_dir,\n",
    "            \"num_train_epochs\": self.num_train_epochs,\n",
    "            \"per_device_train_batch_size\": self.per_device_train_batch_size,\n",
    "            \"per_device_eval_batch_size\": self.per_device_eval_batch_size,\n",
    "            \"gradient_accumulation_steps\": self.gradient_accumulation_steps,\n",
    "            \"learning_rate\": self.learning_rate,\n",
    "            \"weight_decay\": self.weight_decay,\n",
    "            \"warmup_steps\": self.warmup_steps,\n",
    "            \"lr_scheduler_type\": self.lr_scheduler_type,\n",
    "            \"fp16\": self.fp16,\n",
    "            \"bf16\": self.bf16,\n",
    "            \"gradient_checkpointing\": self.gradient_checkpointing,\n",
    "            \"dataloader_pin_memory\": self.dataloader_pin_memory,\n",
    "            \"eval_strategy\": self.eval_strategy,\n",
    "            \"eval_steps\": self.eval_steps,\n",
    "            \"logging_steps\": self.logging_steps,\n",
    "            \"save_strategy\": self.save_strategy,\n",
    "            \"save_steps\": self.save_steps,\n",
    "            \"save_total_limit\": self.save_total_limit,\n",
    "            \"load_best_model_at_end\": self.load_best_model_at_end,\n",
    "            \"metric_for_best_model\": self.metric_for_best_model,\n",
    "            \"greater_is_better\": self.greater_is_better,\n",
    "            \"remove_unused_columns\": self.remove_unused_columns,\n",
    "            \"dataloader_num_workers\": self.dataloader_num_workers,\n",
    "            \"report_to\": \"none\",  # Disable wandb/tensorboard by default\n",
    "            \"seed\": 42,\n",
    "        }\n",
    "        \n",
    "        if max_steps is not None:\n",
    "            args_dict[\"max_steps\"] = max_steps\n",
    "            args_dict.pop(\"num_train_epochs\")  # Remove epochs if using max_steps\n",
    "        \n",
    "        return TrainingArguments(**args_dict)\n",
    "    \n",
    "    def get_memory_efficient_args(self, max_steps: Optional[int] = None) -> TrainingArguments:\n",
    "        \"\"\"Get memory-efficient training arguments\"\"\"\n",
    "        # Reduce batch sizes for memory efficiency\n",
    "        self.per_device_train_batch_size = 2\n",
    "        self.per_device_eval_batch_size = 2\n",
    "        self.gradient_accumulation_steps = 8  # Keep effective batch size = 16\n",
    "        self.gradient_checkpointing = True\n",
    "        self.dataloader_pin_memory = False\n",
    "        \n",
    "        return self.get_training_args(max_steps)\n",
    "    \n",
    "    def get_high_performance_args(self, max_steps: Optional[int] = None) -> TrainingArguments:\n",
    "        \"\"\"Get high-performance training arguments\"\"\"\n",
    "        # Increase batch sizes for better performance\n",
    "        self.per_device_train_batch_size = 4\n",
    "        self.per_device_eval_batch_size = 4\n",
    "        self.gradient_accumulation_steps = 8  # Effective batch size = 16\n",
    "        self.dataloader_num_workers = 4\n",
    "        self.dataloader_pin_memory = True\n",
    "        \n",
    "        return self.get_training_args(max_steps)\n",
    "    \n",
    "    def calculate_training_steps(self, dataset_size: int) -> dict:\n",
    "        \"\"\"Calculate training steps and duration\"\"\"\n",
    "        effective_batch_size = (\n",
    "            self.per_device_train_batch_size * \n",
    "            self.gradient_accumulation_steps\n",
    "        )\n",
    "        \n",
    "        steps_per_epoch = dataset_size // effective_batch_size\n",
    "        total_steps = steps_per_epoch * self.num_train_epochs\n",
    "        \n",
    "        # Estimate training time (rough approximation)\n",
    "        # Assuming ~1.5 seconds per step on RTX 4090\n",
    "        estimated_time_hours = (total_steps * 1.5) / 3600\n",
    "        \n",
    "        return {\n",
    "            \"dataset_size\": dataset_size,\n",
    "            \"effective_batch_size\": effective_batch_size,\n",
    "            \"steps_per_epoch\": steps_per_epoch,\n",
    "            \"total_training_steps\": total_steps,\n",
    "            \"estimated_time_hours\": estimated_time_hours,\n",
    "            \"warmup_ratio\": self.warmup_steps / total_steps if total_steps > 0 else 0\n",
    "        }\n",
    "    \n",
    "    def print_config_summary(self, dataset_size: Optional[int] = None):\n",
    "        \"\"\"Print training configuration summary\"\"\"\n",
    "        print(\"=== Training Configuration Summary ===\")\n",
    "        print(f\"Learning rate: {self.learning_rate}\")\n",
    "        print(f\"Batch size per device: {self.per_device_train_batch_size}\")\n",
    "        print(f\"Gradient accumulation: {self.gradient_accumulation_steps}\")\n",
    "        print(f\"Effective batch size: {self.per_device_train_batch_size * self.gradient_accumulation_steps}\")\n",
    "        print(f\"Epochs: {self.num_train_epochs}\")\n",
    "        print(f\"Warmup steps: {self.warmup_steps}\")\n",
    "        print(f\"Weight decay: {self.weight_decay}\")\n",
    "        print(f\"Precision: {'BF16' if self.bf16 else 'FP16' if self.fp16 else 'FP32'}\")\n",
    "        print(f\"Gradient checkpointing: {self.gradient_checkpointing}\")\n",
    "        \n",
    "        if dataset_size:\n",
    "            training_info = self.calculate_training_steps(dataset_size)\n",
    "            print(f\"\\n=== Training Schedule ===\")\n",
    "            print(f\"Dataset size: {training_info['dataset_size']}\")\n",
    "            print(f\"Steps per epoch: {training_info['steps_per_epoch']}\")\n",
    "            print(f\"Total training steps: {training_info['total_training_steps']}\")\n",
    "            print(f\"Estimated time: {training_info['estimated_time_hours']:.1f} hours\")\n",
    "            print(f\"Warmup ratio: {training_info['warmup_ratio']:.3f}\")\n",
    "\n",
    "def get_recommended_training_config(available_vram_gb: float, dataset_size: int) -> TrainingArguments:\n",
    "    \"\"\"Get recommended training config based on available VRAM\"\"\"\n",
    "    config_manager = TrainingConfigManager()\n",
    "    \n",
    "    # Calculate max steps for better control\n",
    "    training_info = config_manager.calculate_training_steps(dataset_size)\n",
    "    max_steps = training_info['total_training_steps']\n",
    "    \n",
    "    if available_vram_gb >= 16:\n",
    "        print(\"Using high-performance training configuration\")\n",
    "        return config_manager.get_high_performance_args(max_steps)\n",
    "    elif available_vram_gb >= 12:\n",
    "        print(\"Using standard training configuration\")\n",
    "        return config_manager.get_training_args(max_steps)\n",
    "    else:\n",
    "        print(\"Using memory-efficient training configuration\")\n",
    "        return config_manager.get_memory_efficient_args(max_steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compat + Memory-Efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T17:17:13.490827Z",
     "iopub.status.busy": "2025-09-17T17:17:13.490487Z",
     "iopub.status.idle": "2025-09-17T17:17:13.536226Z",
     "shell.execute_reply": "2025-09-17T17:17:13.535300Z",
     "shell.execute_reply.started": "2025-09-17T17:17:13.490796Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected dataset_size = 20434\n",
      "Available VRAM ≈ 17.1 GB\n",
      "Using FP16 precision\n",
      "=== Training Configuration Summary ===\n",
      "Learning rate: 0.0002\n",
      "Batch size per device: 4\n",
      "Gradient accumulation: 4\n",
      "Effective batch size: 16\n",
      "Epochs: 3\n",
      "Warmup steps: 100\n",
      "Weight decay: 0.01\n",
      "Precision: FP16\n",
      "Gradient checkpointing: True\n",
      "\n",
      "=== Training Schedule ===\n",
      "Dataset size: 20434\n",
      "Steps per epoch: 1277\n",
      "Total training steps: 3831\n",
      "Estimated time: 1.6 hours\n",
      "Warmup ratio: 0.026\n",
      "\n",
      "== Preview key TrainingArguments ==\n",
      "output_dir: ./qwen-banking-lora\n",
      "per_device_train_batch_size: 1\n",
      "gradient_accumulation_steps: 16\n",
      "learning_rate: 0.0002\n",
      "eval/log/save steps: 100 10 200\n",
      "precision fp16/bf16: True False\n",
      "optim: OptimizerNames.PAGED_ADAMW_8BIT\n"
     ]
    }
   ],
   "source": [
    "# === Compat + Memory-Efficient TrainingArguments builder ===\n",
    "import os, json, torch\n",
    "from dataclasses import fields as dataclass_fields\n",
    "from transformers import TrainingArguments as HFTrainingArguments\n",
    "\n",
    "# 1) dataset_size\n",
    "if 'train_data' in globals():\n",
    "    dataset_size = len(train_data)\n",
    "else:\n",
    "    dataset_size = 0\n",
    "    guess = \"/kaggle/working/data/processed/train_split/train.jsonl\"\n",
    "    if os.path.exists(guess):\n",
    "        with open(guess, \"r\", encoding=\"utf-8\") as f:\n",
    "            for _ in f:\n",
    "                dataset_size += 1\n",
    "    else:\n",
    "        dataset_size = 1000\n",
    "print(f\"Detected dataset_size = {dataset_size}\")\n",
    "\n",
    "# 2) VRAM\n",
    "available_vram = 8.0\n",
    "if torch.cuda.is_available():\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    available_vram = props.total_memory / 1e9\n",
    "print(f\"Available VRAM ≈ {available_vram:.1f} GB\")\n",
    "\n",
    "# 3) config summary\n",
    "tcm = TrainingConfigManager()\n",
    "tcm.print_config_summary(dataset_size=dataset_size)\n",
    "\n",
    "# 4) ÉP profile memory-efficient (tránh BS=8)\n",
    "#    Giữ đúng logic function get_memory_efficient_args của bạn:\n",
    "tcm.per_device_train_batch_size = 1    # mạnh tay để chắc chắn fit\n",
    "tcm.per_device_eval_batch_size = 1\n",
    "tcm.gradient_accumulation_steps = 16   # effective batch vẫn 16\n",
    "tcm.gradient_checkpointing = True\n",
    "tcm.dataloader_pin_memory = False\n",
    "\n",
    "# 5) Dùng AdamW 8-bit (giảm RAM) nếu transformers hỗ trợ qua TrainingArguments\n",
    "extra_optim = {\"optim\": \"paged_adamw_8bit\"}  # cần bitsandbytes\n",
    "\n",
    "# 6) Build args_dict giống hệt get_training_args\n",
    "args_dict = {\n",
    "    \"output_dir\": tcm.output_dir,\n",
    "    \"num_train_epochs\": tcm.num_train_epochs,\n",
    "    \"per_device_train_batch_size\": tcm.per_device_train_batch_size,\n",
    "    \"per_device_eval_batch_size\": tcm.per_device_eval_batch_size,\n",
    "    \"gradient_accumulation_steps\": tcm.gradient_accumulation_steps,\n",
    "    \"learning_rate\": tcm.learning_rate,\n",
    "    \"weight_decay\": tcm.weight_decay,\n",
    "    \"warmup_steps\": tcm.warmup_steps,\n",
    "    \"lr_scheduler_type\": tcm.lr_scheduler_type,\n",
    "    \"fp16\": tcm.fp16,\n",
    "    \"bf16\": tcm.bf16,\n",
    "    \"gradient_checkpointing\": tcm.gradient_checkpointing,\n",
    "    \"dataloader_pin_memory\": tcm.dataloader_pin_memory,\n",
    "    \"eval_strategy\": tcm.eval_strategy,\n",
    "    \"eval_steps\": tcm.eval_steps,\n",
    "    \"logging_steps\": tcm.logging_steps,\n",
    "    \"save_strategy\": tcm.save_strategy,\n",
    "    \"save_steps\": tcm.save_steps,\n",
    "    \"save_total_limit\": tcm.save_total_limit,\n",
    "    \"load_best_model_at_end\": tcm.load_best_model_at_end,\n",
    "    \"metric_for_best_model\": tcm.metric_for_best_model,\n",
    "    \"greater_is_better\": tcm.greater_is_better,\n",
    "    \"remove_unused_columns\": tcm.remove_unused_columns,\n",
    "    \"dataloader_num_workers\": tcm.dataloader_num_workers,\n",
    "    \"report_to\": \"none\",\n",
    "    \"seed\": 42,\n",
    "    **extra_optim,\n",
    "}\n",
    "\n",
    "# dùng max_steps như trước\n",
    "sched = tcm.calculate_training_steps(dataset_size)\n",
    "max_steps = sched['total_training_steps']\n",
    "if max_steps and max_steps > 0:\n",
    "    args_dict[\"max_steps\"] = max_steps\n",
    "    args_dict.pop(\"num_train_epochs\", None)\n",
    "\n",
    "# 7) lọc field theo version transformers hiện tại\n",
    "ta_field_names = {f.name for f in dataclass_fields(HFTrainingArguments)}\n",
    "if \"eval_strategy\" not in ta_field_names and \"eval_strategy\" in ta_field_names:\n",
    "    args_dict[\"eval_strategy\"] = args_dict.pop(\"eval_strategy\", \"steps\")\n",
    "filtered_args = {k: v for k, v in args_dict.items() if k in ta_field_names}\n",
    "\n",
    "training_args = HFTrainingArguments(**filtered_args)\n",
    "\n",
    "print(\"\\n== Preview key TrainingArguments ==\")\n",
    "print(\"output_dir:\", training_args.output_dir)\n",
    "print(\"per_device_train_batch_size:\", training_args.per_device_train_batch_size)\n",
    "print(\"gradient_accumulation_steps:\", training_args.gradient_accumulation_steps)\n",
    "print(\"learning_rate:\", training_args.learning_rate)\n",
    "print(\"eval/log/save steps:\",\n",
    "      getattr(training_args, \"eval_steps\", None),\n",
    "      getattr(training_args, \"logging_steps\", None),\n",
    "      getattr(training_args, \"save_steps\", None))\n",
    "print(\"precision fp16/bf16:\", getattr(training_args, \"fp16\", None), getattr(training_args, \"bf16\", None))\n",
    "print(\"optim:\", getattr(training_args, \"optim\", None))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f79bd45d-7e9f-495b-a9c6-efb03345503c",
    "_uuid": "958644e3-5b73-40c5-b596-a4bb2eb5d186",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Session 4 — Fine-tune Qwen\n",
    "Import `QwenBankingFineTuner` từ `fine_tune_qwen.py` (không chạy `main()`), rồi chạy train bằng đường dẫn Kaggle.\n",
    "\n",
    "> **Ghi chú:** Nếu không bật GPU, tham số `attn_implementation='flash_attention_2'` trong code chỉ được set khi CUDA khả dụng. Nếu dùng CPU-only, bạn vẫn có thể import lớp và huấn luyện chậm hơn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T17:17:13.537664Z",
     "iopub.status.busy": "2025-09-17T17:17:13.537205Z",
     "iopub.status.idle": "2025-09-17T17:17:40.617256Z",
     "shell.execute_reply": "2025-09-17T17:17:40.615970Z",
     "shell.execute_reply.started": "2025-09-17T17:17:13.537634Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip -q install \"flash-attn>=2.5.9\" --no-build-isolation || echo \"flash-attn install failed; will fallback to SDPA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "dd47899c-9bb6-4906-a973-c5ff19dc5371",
    "_uuid": "4aca1324-b9bd-47c7-9518-e89a3c25356e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-17T17:17:40.619342Z",
     "iopub.status.busy": "2025-09-17T17:17:40.618960Z",
     "iopub.status.idle": "2025-09-17T17:17:43.697191Z",
     "shell.execute_reply": "2025-09-17T17:17:43.696290Z",
     "shell.execute_reply.started": "2025-09-17T17:17:40.619298Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fine-tuning Script for Qwen3-4B Banking Chatbot\n",
    "Script chính để fine-tune model với LoRA\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Trainer\n",
    ")\n",
    "from peft import get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import Dataset\n",
    "from typing import Dict, List, Any\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# === LƯU Ý ===\n",
    "# Trong notebook, bạn đã có:\n",
    "#  - LoRAConfigManager, get_recommended_config (từ phần LoRA)\n",
    "#  - TrainingConfigManager, get_recommended_training_config (từ phần Training)\n",
    "# Nếu muốn import như file .py: hãy %%writefile lora_config.py & training_config.py trước, rồi mở 2 dòng dưới:\n",
    "# from lora_config import LoRAConfigManager, get_recommended_config\n",
    "# from training_config import TrainingConfigManager, get_recommended_training_config\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class QwenBankingFineTuner:\n",
    "    def __init__(self, \n",
    "                 model_name: str = \"Qwen/Qwen3-4B\",\n",
    "                 data_dir: str = \"/kaggle/working/data/processed/train_split\",\n",
    "                 output_dir: str = \"/kaggle/working/qwen-banking-lora\"):\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.data_dir = data_dir\n",
    "        self.output_dir = output_dir\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Initialize components\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.train_dataset = None\n",
    "        self.eval_dataset = None\n",
    "        \n",
    "        logger.info(f\"Initializing fine-tuner for {model_name}\")\n",
    "        logger.info(f\"Device: {self.device}\")\n",
    "        if torch.cuda.is_available():\n",
    "            logger.info(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "            logger.info(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    \n",
    "    def load_tokenizer_and_model(self):\n",
    "        \"\"\"Load tokenizer and model\"\"\"\n",
    "        logger.info(\"Loading tokenizer and model...\")\n",
    "        \n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.model_name,\n",
    "            trust_remote_code=True,\n",
    "            padding_side=\"right\"\n",
    "        )\n",
    "        \n",
    "        # Add pad token if not exists\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Load model with optimizations\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            attn_implementation=\"flash_attention_2\" if torch.cuda.is_available() else None\n",
    "        )\n",
    "        \n",
    "        # Prepare model for LoRA\n",
    "        self.model = prepare_model_for_kbit_training(self.model)\n",
    "        \n",
    "        logger.info(\"Model and tokenizer loaded successfully\")\n",
    "    \n",
    "    def load_datasets(self):\n",
    "        \"\"\"Load training and validation datasets\"\"\"\n",
    "        logger.info(\"Loading datasets...\")\n",
    "        \n",
    "        def load_jsonl(filepath: str) -> List[Dict[str, Any]]:\n",
    "            data = []\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    data.append(json.loads(line.strip()))\n",
    "            return data\n",
    "        \n",
    "        # Load data\n",
    "        train_data = load_jsonl(os.path.join(self.data_dir, \"train.jsonl\"))\n",
    "        val_data = load_jsonl(os.path.join(self.data_dir, \"validation.jsonl\"))\n",
    "        \n",
    "        # Convert to datasets\n",
    "        self.train_dataset = Dataset.from_list(train_data)\n",
    "        self.eval_dataset = Dataset.from_list(val_data)\n",
    "        \n",
    "        logger.info(f\"Loaded {len(self.train_dataset)} training samples\")\n",
    "        logger.info(f\"Loaded {len(self.eval_dataset)} validation samples\")\n",
    "    \n",
    "    def preprocess_function(self, examples):\n",
    "        \"\"\"Preprocess examples for training\"\"\"\n",
    "        model_inputs = {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
    "        \n",
    "        for messages in examples[\"messages\"]:\n",
    "            # Apply chat template\n",
    "            text = self.tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=False\n",
    "            )\n",
    "            \n",
    "            # Tokenize\n",
    "            tokenized = self.tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                max_length=1024,\n",
    "                padding=False,\n",
    "                return_tensors=None\n",
    "            )\n",
    "            \n",
    "            model_inputs[\"input_ids\"].append(tokenized[\"input_ids\"])\n",
    "            model_inputs[\"attention_mask\"].append(tokenized[\"attention_mask\"])\n",
    "            \n",
    "            # Labels are the same as input_ids for causal LM\n",
    "            model_inputs[\"labels\"].append(tokenized[\"input_ids\"].copy())\n",
    "        \n",
    "        return model_inputs\n",
    "    \n",
    "    def setup_lora(self, available_vram_gb: float = 12.0):\n",
    "        \"\"\"Setup LoRA configuration\"\"\"\n",
    "        logger.info(\"Setting up LoRA...\")\n",
    "        \n",
    "        # Get recommended LoRA config (đã định nghĩa ở session LoRA)\n",
    "        lora_config = get_recommended_config(available_vram_gb)\n",
    "        \n",
    "        # Apply LoRA to model\n",
    "        self.model = get_peft_model(self.model, lora_config)\n",
    "        \n",
    "        # Print trainable parameters\n",
    "        self.model.print_trainable_parameters()\n",
    "        \n",
    "        logger.info(\"LoRA setup completed\")\n",
    "    \n",
    "    def train(self, available_vram_gb: float = 12.0):\n",
    "        \"\"\"Main training function\"\"\"\n",
    "        logger.info(\"Starting training...\")\n",
    "        \n",
    "        # Preprocess datasets\n",
    "        train_dataset = self.train_dataset.map(\n",
    "            self.preprocess_function,\n",
    "            batched=True,\n",
    "            remove_columns=self.train_dataset.column_names\n",
    "        )\n",
    "        \n",
    "        eval_dataset = self.eval_dataset.map(\n",
    "            self.preprocess_function,\n",
    "            batched=True,\n",
    "            remove_columns=self.eval_dataset.column_names\n",
    "        )\n",
    "        \n",
    "        # Get training arguments (đã định nghĩa ở session Training)\n",
    "        training_args = get_recommended_training_config(\n",
    "            available_vram_gb, \n",
    "            len(train_dataset)\n",
    "        )\n",
    "        training_args.output_dir = self.output_dir\n",
    "        \n",
    "        # Data collator\n",
    "        data_collator = DataCollatorForSeq2Seq(\n",
    "            tokenizer=self.tokenizer,\n",
    "            model=self.model,\n",
    "            padding=True,\n",
    "            pad_to_multiple_of=8,   # thêm dòng này\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Initialize trainer\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            tokenizer=self.tokenizer,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "        \n",
    "        # Start training\n",
    "        start_time = datetime.now()\n",
    "        logger.info(f\"Training started at {start_time}\")\n",
    "        \n",
    "        trainer.train()\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        training_duration = end_time - start_time\n",
    "        logger.info(f\"Training completed at {end_time}\")\n",
    "        logger.info(f\"Training duration: {training_duration}\")\n",
    "        \n",
    "        # Save the final model\n",
    "        trainer.save_model()\n",
    "        self.tokenizer.save_pretrained(self.output_dir)\n",
    "        \n",
    "        logger.info(f\"Model saved to {self.output_dir}\")\n",
    "        \n",
    "        return trainer\n",
    "    \n",
    "    def run_fine_tuning(self, available_vram_gb: float = 12.0):\n",
    "        \"\"\"Complete fine-tuning pipeline\"\"\"\n",
    "        try:\n",
    "            # Load model and tokenizer\n",
    "            self.load_tokenizer_and_model()\n",
    "            \n",
    "            # Load datasets\n",
    "            self.load_datasets()\n",
    "            \n",
    "            # Setup LoRA\n",
    "            self.setup_lora(available_vram_gb)\n",
    "            \n",
    "            # Train model\n",
    "            trainer = self.train(available_vram_gb)\n",
    "            \n",
    "            logger.info(\"Fine-tuning completed successfully!\")\n",
    "            return trainer\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during fine-tuning: {str(e)}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **QLoRA 4-bit hay fp16 fallback**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T17:17:43.699347Z",
     "iopub.status.busy": "2025-09-17T17:17:43.698275Z",
     "iopub.status.idle": "2025-09-17T17:17:43.709457Z",
     "shell.execute_reply": "2025-09-17T17:17:43.708614Z",
     "shell.execute_reply.started": "2025-09-17T17:17:43.699316Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Safe patch: ưu tiên QLoRA 4-bit, nếu fail thử 8-bit, cuối cùng fallback fp16\n",
    "import torch, logging\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def _safe_load_tokenizer_and_model(self):\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.info(\"Loading tokenizer/model with auto QLoRA(4b)->8b->fp16 fallback...\")\n",
    "\n",
    "    # Tokenizer\n",
    "    self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "        self.model_name, trust_remote_code=True, padding_side=\"right\"\n",
    "    )\n",
    "    if self.tokenizer.pad_token is None:\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "    # Attention backend\n",
    "    attn_impl = None\n",
    "    if torch.cuda.is_available():\n",
    "        try:\n",
    "            import flash_attn  # noqa\n",
    "            attn_impl = \"flash_attention_2\"\n",
    "            logger.info(\"FlashAttention-2 detected.\")\n",
    "        except Exception:\n",
    "            attn_impl = \"sdpa\"\n",
    "            logger.info(\"Using SDPA attention.\")\n",
    "    self.attn_impl_chosen = attn_impl\n",
    "\n",
    "    # Thử import BnB & BitsAndBytesConfig\n",
    "    use_bnb = False\n",
    "    try:\n",
    "        from transformers import BitsAndBytesConfig\n",
    "        import bitsandbytes as bnb  # noqa\n",
    "        use_bnb = True\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"bitsandbytes/BitsAndBytesConfig not available -> fallback fp16. Detail: {e}\")\n",
    "\n",
    "    # 4-bit -> 8-bit -> fp16\n",
    "    if use_bnb:\n",
    "        # Ưu tiên 4-bit\n",
    "        try:\n",
    "            bnb_cfg = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16\n",
    "            )\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True,\n",
    "                quantization_config=bnb_cfg,\n",
    "                attn_implementation=attn_impl\n",
    "            )\n",
    "            self.load_mode = \"4bit\"\n",
    "            logger.info(\"Loaded model with QLoRA (4-bit).\")\n",
    "        except Exception as e4:\n",
    "            logger.warning(f\"QLoRA 4-bit failed -> try 8-bit. Detail: {e4}\")\n",
    "            # Thử 8-bit\n",
    "            try:\n",
    "                bnb_cfg8 = BitsAndBytesConfig(load_in_8bit=True)\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    self.model_name,\n",
    "                    device_map=\"auto\",\n",
    "                    trust_remote_code=True,\n",
    "                    quantization_config=bnb_cfg8,\n",
    "                    attn_implementation=attn_impl\n",
    "                )\n",
    "                self.load_mode = \"8bit\"\n",
    "                logger.info(\"Loaded model with 8-bit quantization.\")\n",
    "            except Exception as e8:\n",
    "                logger.warning(f\"8-bit failed -> fallback fp16. Detail: {e8}\")\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    self.model_name,\n",
    "                    torch_dtype=torch.float16,\n",
    "                    device_map=\"auto\",\n",
    "                    trust_remote_code=True,\n",
    "                    attn_implementation=attn_impl\n",
    "                )\n",
    "                self.load_mode = \"fp16\"\n",
    "                logger.info(\"Loaded model in fp16.\")\n",
    "    else:\n",
    "        # Không có BnB -> fp16\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            attn_implementation=attn_impl\n",
    "        )\n",
    "        self.load_mode = \"fp16\"\n",
    "        logger.info(\"Loaded model in fp16.\")\n",
    "\n",
    "    # Hợp gradient checkpointing\n",
    "    if hasattr(self.model, \"config\") and getattr(self.model.config, \"use_cache\", False):\n",
    "        self.model.config.use_cache = False\n",
    "\n",
    "    # Chuẩn bị cho k-bit training (OK cho 4b/8b/fp16)\n",
    "    from peft import prepare_model_for_kbit_training\n",
    "    self.model = prepare_model_for_kbit_training(self.model)\n",
    "    logger.info(f\"Model ready. load_mode={self.load_mode}, attn_impl={self.attn_impl_chosen}\")\n",
    "\n",
    "QwenBankingFineTuner.load_tokenizer_and_model = _safe_load_tokenizer_and_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T17:17:43.710726Z",
     "iopub.status.busy": "2025-09-17T17:17:43.710376Z",
     "iopub.status.idle": "2025-09-17T17:17:44.562487Z",
     "shell.execute_reply": "2025-09-17T17:17:44.561557Z",
     "shell.execute_reply.started": "2025-09-17T17:17:43.710700Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU cache cleared.\n"
     ]
    }
   ],
   "source": [
    "import gc, torch\n",
    "for obj in [\"trainer\", \"fine_tuner\"]:\n",
    "    if obj in globals():\n",
    "        try:\n",
    "            del globals()[obj]\n",
    "        except: pass\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    # giảm phân mảnh bộ nhớ:\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "print(\"GPU cache cleared.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Cấu hình Kaggle & chạy fine-tuning (thay cho main())**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "eb259274-24b1-4361-95e7-1c9c37d2d1e4",
    "_uuid": "ad2bb489-795e-40b2-a619-2a24c9b1ea66",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-17T17:21:48.466550Z",
     "iopub.status.busy": "2025-09-17T17:21:48.466168Z",
     "iopub.status.idle": "2025-09-17T17:21:49.502239Z",
     "shell.execute_reply": "2025-09-17T17:21:49.501057Z",
     "shell.execute_reply.started": "2025-09-17T17:21:48.466489Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36/2561111784.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mfine_tuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"4bit\"\u001b[0m   \u001b[0;31m# QLoRA 4-bit (cần GPU cc >= 7.0: T4/V100/A100)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#fine_tuner.quant_mode = \"8bit\" # 8-bit (ổn nếu 4-bit không chạy hoặc GPU là P100)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfine_tuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_fine_tuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavailable_vram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# (Tuỳ chọn) In một số thống kê cuối\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_36/982986746.py\u001b[0m in \u001b[0;36mrun_fine_tuning\u001b[0;34m(self, available_vram_gb)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;31m# Load model and tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_tokenizer_and_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;31m# Load datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_36/447623975.py\u001b[0m in \u001b[0;36m_enforced_quant_loader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m# 6) Load model với quant tương ứng (không fp16)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     self.model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    572\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4389\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4390\u001b[0;31m             hf_quantizer.validate_environment(\n\u001b[0m\u001b[1;32m   4391\u001b[0m                 \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4392\u001b[0m                 \u001b[0mfrom_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_tf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/quantizers/quantizer_bnb_4bit.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             )\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_bitsandbytes_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             raise ImportError(\n\u001b[0m\u001b[1;32m     77\u001b[0m                 \u001b[0;34m\"Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             )\n",
      "\u001b[0;31mImportError\u001b[0m: Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`"
     ]
    }
   ],
   "source": [
    "# Cấu hình cho Kaggle\n",
    "MODEL_NAME = \"Qwen/Qwen3-4B\"\n",
    "DATA_DIR   = \"/kaggle/working/data/processed/train_split\"  # đã tạo ở bước tiền xử lý\n",
    "OUTPUT_DIR = \"/kaggle/working/qwen-banking-lora\"\n",
    "\n",
    "# Phát hiện VRAM khả dụng\n",
    "if torch.cuda.is_available():\n",
    "    available_vram = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    logger.info(f\"Available VRAM: {available_vram:.1f} GB\")\n",
    "else:\n",
    "    available_vram = 8.0\n",
    "    logger.warning(\"CUDA not available, using CPU\")\n",
    "\n",
    "# Khởi tạo & chạy fine-tuning\n",
    "fine_tuner = QwenBankingFineTuner(\n",
    "    model_name=MODEL_NAME,\n",
    "    data_dir=DATA_DIR,\n",
    "    output_dir=OUTPUT_DIR\n",
    ")\n",
    "\n",
    "trainer = fine_tuner.run_fine_tuning(available_vram)\n",
    "\n",
    "# (Tuỳ chọn) In một số thống kê cuối\n",
    "if trainer:\n",
    "    hist = trainer.state.log_history\n",
    "    # Tìm log loss cuối cùng nếu có\n",
    "    final_train_loss = next((d.get('train_loss') for d in reversed(hist) if 'train_loss' in d), 'N/A')\n",
    "    final_eval_loss  = next((d.get('eval_loss') for d in reversed(hist) if 'eval_loss' in d), 'N/A')\n",
    "    print(\"\\n=== Training Statistics ===\")\n",
    "    print(\"Final training loss:\", final_train_loss)\n",
    "    print(\"Final eval loss:\", final_eval_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T16:27:00.902958Z",
     "iopub.status.busy": "2025-09-17T16:27:00.902666Z",
     "iopub.status.idle": "2025-09-17T16:27:00.909705Z",
     "shell.execute_reply": "2025-09-17T16:27:00.908933Z",
     "shell.execute_reply.started": "2025-09-17T16:27:00.902934Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.6.0+cu124\n",
      "cuda available: True\n",
      "device: Tesla P100-PCIE-16GB\n",
      "capability: (6, 0)\n",
      "bitsandbytes: 0.47.0\n",
      "Linear4bit init: OK -> 4-bit likely supported\n"
     ]
    }
   ],
   "source": [
    "import torch, platform\n",
    "\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"device:\", torch.cuda.get_device_name(0))\n",
    "    print(\"capability:\", torch.cuda.get_device_capability(0))  # (major, minor)\n",
    "\n",
    "try:\n",
    "    import bitsandbytes as bnb\n",
    "    from bitsandbytes.nn import Linear4bit\n",
    "    print(\"bitsandbytes:\", bnb.__version__)\n",
    "    # Thử khởi tạo layer 4-bit\n",
    "    test = Linear4bit(16, 16, quant_type='nf4', compute_dtype=torch.float16)\n",
    "    print(\"Linear4bit init: OK -> 4-bit likely supported\")\n",
    "    del test\n",
    "    fourbit_ok = True\n",
    "except Exception as e:\n",
    "    print(\"4-bit check failed:\", repr(e))\n",
    "    fourbit_ok = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_quantization_status(ft):\n",
    "    print(\"load_mode:\", getattr(ft, \"load_mode\", \"<unknown>\"))\n",
    "    print(\"attn_impl:\", getattr(ft, \"attn_impl_chosen\", \"<unknown>\"))\n",
    "    qc = getattr(ft.model, \"quantization_config\", None)\n",
    "    print(\"has quantization_config:\", qc is not None)\n",
    "    if qc:\n",
    "        for k in [\"load_in_4bit\",\"bnb_4bit_quant_type\",\"bnb_4bit_use_double_quant\",\"bnb_4bit_compute_dtype\"]:\n",
    "            print(f\"  {k}:\", getattr(qc, k, None))\n",
    "    try:\n",
    "        from bitsandbytes.nn import Linear4bit\n",
    "        has_4bit = any(isinstance(m, Linear4bit) for m in ft.model.modules())\n",
    "        print(\"contains Linear4bit layers:\", has_4bit)\n",
    "    except Exception as e:\n",
    "        print(\"bitsandbytes check skipped:\", e)\n",
    "\n",
    "print_quantization_status(fine_tuner)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8288514,
     "sourceId": 13086163,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

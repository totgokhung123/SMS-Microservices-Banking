{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3bd719c5-d39a-47c1-921f-d589de7e238c",
    "_uuid": "56622a65-0c8c-4e5c-bd50-823b1778245a",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "# Qwen/Qwen3-4B Banking Chatbot â€” Kaggle Notebook\n",
    "\n",
    "Notebook nÃ y gom **Ä‘Ãºng logic & tham sá»‘** tá»« 4 file cá»§a báº¡n theo thá»© tá»±:\n",
    "1) `data_preprocessing` â†’ 2) `lora_config` â†’ 3) `training_config` â†’ 4) `fine_tune_qwen`.\n",
    "\n",
    "**LÆ°u Ã½:**\n",
    "- MÃ¬nh giá»¯ nguyÃªn code gá»‘c (khÃ´ng Ä‘á»•i logic/hyperparams). CÃ¡c Ä‘Æ°á»ng dáº«n Ä‘Æ°á»£c cáº¥u hÃ¬nh báº±ng biáº¿n trong notebook Ä‘á»ƒ cháº¡y trÃªn Kaggle.\n",
    "- Báº¡n cáº§n **báº­t GPU** trong Kaggle (Settings â†’ Accelerator: GPU) trÆ°á»›c khi train."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "_cell_guid": "1d656d82-cc7e-4030-be3c-3be7a09067c9",
    "_uuid": "b476f23b-0b8d-456c-9ced-3e38530ad429",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-17T17:15:35.754645Z",
     "iopub.status.busy": "2025-09-17T17:15:35.754355Z",
     "iopub.status.idle": "2025-09-17T17:16:59.704379Z",
     "shell.execute_reply": "2025-09-17T17:16:59.703558Z",
     "shell.execute_reply.started": "2025-09-17T17:15:35.754621Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install transformers datasets peft accelerate scikit-learn tokenizers einops"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T17:16:59.707169Z",
     "iopub.status.busy": "2025-09-17T17:16:59.706912Z",
     "iopub.status.idle": "2025-09-17T17:17:08.288035Z",
     "shell.execute_reply": "2025-09-17T17:17:08.286646Z",
     "shell.execute_reply.started": "2025-09-17T17:16:59.707143Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "2d08a541-4f50-4211-8931-2ef218ebdf3e",
    "_uuid": "1bdee4ba-6513-49d7-bea9-cd269fd49a36",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Session 1 â€” Data Preprocessing\n",
    "Thiáº¿t láº­p Ä‘Æ°á»ng dáº«n Kaggle vÃ  cháº¡y pipeline tiá»n xá»­ lÃ½ Ä‘á»ƒ xuáº¥t `train.jsonl`, `validation.jsonl`, `test.jsonl` vÃ  `dataset_stats.json`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "f8330dd3-a33d-4978-9b71-55507b1417db",
    "_uuid": "7930ca13-1f13-4801-b777-e589e8c65a37",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-17T17:17:08.290127Z",
     "iopub.status.busy": "2025-09-17T17:17:08.289397Z",
     "iopub.status.idle": "2025-09-17T17:17:08.363097Z",
     "shell.execute_reply": "2025-09-17T17:17:08.361956Z",
     "shell.execute_reply.started": "2025-09-17T17:17:08.290076Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Data Preprocessing Script for Qwen2-4B Banking Chatbot Fine-tuning\n",
    "# Xá»­ lÃ½ dá»¯ liá»‡u CSV thÃ nh format ChatML phÃ¹ há»£p vá»›i Qwen2-4B\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split\n",
    "from typing import List, Dict, Any\n",
    "import logging\n",
    "\n",
    "# Setup logging\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class BankingDataPreprocessor:\n",
    "    def __init__(self, csv_path: str, output_dir: str):\n",
    "        self.csv_path = csv_path\n",
    "        self.output_dir = output_dir\n",
    "        self.system_prompt = \"\"\"Báº¡n lÃ  trá»£ lÃ½ tÆ° váº¥n tÃ i chÃ­nh ngÃ¢n hÃ ng chuyÃªn nghiá»‡p cá»§a HDBank. Báº¡n cÃ³ kiáº¿n thá»©c sÃ¢u vá» cÃ¡c sáº£n pháº©m vÃ  dá»‹ch vá»¥ ngÃ¢n hÃ ng, luÃ´n há»— trá»£ khÃ¡ch hÃ ng má»™t cÃ¡ch táº­n tÃ¬nh vÃ  chÃ­nh xÃ¡c. HÃ£y tráº£ lá»i cÃ¡c cÃ¢u há»i má»™t cÃ¡ch chi tiáº¿t, dá»… hiá»ƒu vÃ  thÃ¢n thiá»‡n.\"\"\"\n",
    "        \n",
    "    def load_data(self) -> pd.DataFrame:\n",
    "        \"\"\"Load CSV data\"\"\"\n",
    "        logger.info(f\"Loading data from {self.csv_path}\")\n",
    "        df = pd.read_csv(self.csv_path, encoding='utf-8')\n",
    "        logger.info(f\"Loaded {len(df)} records\")\n",
    "        return df\n",
    "    \n",
    "    def clean_text(self, text: str) -> str:\n",
    "        \"\"\"Clean and normalize text\"\"\"\n",
    "        if pd.isna(text):\n",
    "            return \"\"\n",
    "        \n",
    "        # Remove extra whitespaces and normalize\n",
    "        text = str(text).strip()\n",
    "        text = ' '.join(text.split())\n",
    "        \n",
    "        # Remove quotes that might interfere with JSON\n",
    "        text = text.replace('\"\"', '\"')\n",
    "        \n",
    "        return text\n",
    "    \n",
    "    def create_chat_format(self, instruction: str, response: str) -> Dict[str, Any]:\n",
    "        \"\"\"Convert instruction-response pair to ChatML format\"\"\"\n",
    "        return {\n",
    "            \"messages\": [\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": self.system_prompt\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"user\", \n",
    "                    \"content\": self.clean_text(instruction)\n",
    "                },\n",
    "                {\n",
    "                    \"role\": \"assistant\",\n",
    "                    \"content\": self.clean_text(response)\n",
    "                }\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def process_data(self, df: pd.DataFrame) -> List[Dict[str, Any]]:\n",
    "        \"\"\"Process DataFrame to ChatML format\"\"\"\n",
    "        processed_data = []\n",
    "        \n",
    "        for idx, row in df.iterrows():\n",
    "            if pd.isna(row['instruction']) or pd.isna(row['response']):\n",
    "                logger.warning(f\"Skipping row {idx} due to missing data\")\n",
    "                continue\n",
    "                \n",
    "            chat_data = self.create_chat_format(\n",
    "                instruction=row['instruction'],\n",
    "                response=row['response']\n",
    "            )\n",
    "            processed_data.append(chat_data)\n",
    "            \n",
    "        logger.info(f\"Processed {len(processed_data)} valid records\")\n",
    "        return processed_data\n",
    "    \n",
    "    def split_data(self, data: List[Dict[str, Any]], \n",
    "                   train_ratio: float = 0.8, \n",
    "                   val_ratio: float = 0.1,\n",
    "                   test_ratio: float = 0.1) -> tuple:\n",
    "        \"\"\"Split data into train/validation/test sets\"\"\"\n",
    "        assert abs(train_ratio + val_ratio + test_ratio - 1.0) < 1e-6, \"Ratios must sum to 1.0\"\n",
    "        \n",
    "        # First split: train + val vs test\n",
    "        train_val, test = train_test_split(\n",
    "            data, \n",
    "            test_size=test_ratio, \n",
    "            random_state=42,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        # Second split: train vs val\n",
    "        val_size = val_ratio / (train_ratio + val_ratio)\n",
    "        train, val = train_test_split(\n",
    "            train_val,\n",
    "            test_size=val_size,\n",
    "            random_state=42,\n",
    "            shuffle=True\n",
    "        )\n",
    "        \n",
    "        logger.info(f\"Data split - Train: {len(train)}, Val: {len(val)}, Test: {len(test)}\")\n",
    "        return train, val, test\n",
    "    \n",
    "    def save_jsonl(self, data: List[Dict[str, Any]], filename: str):\n",
    "        \"\"\"Save data in JSONL format\"\"\"\n",
    "        filepath = os.path.join(self.output_dir, filename)\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        \n",
    "        with open(filepath, 'w', encoding='utf-8') as f:\n",
    "            for item in data:\n",
    "                f.write(json.dumps(item, ensure_ascii=False) + '\\n')\n",
    "        \n",
    "        logger.info(f\"Saved {len(data)} records to {filepath}\")\n",
    "    \n",
    "    def generate_statistics(self, data: List[Dict[str, Any]]) -> Dict[str, Any]:\n",
    "        \"\"\"Generate dataset statistics\"\"\"\n",
    "        total_samples = len(data)\n",
    "        \n",
    "        # Calculate text lengths\n",
    "        instruction_lengths = []\n",
    "        response_lengths = []\n",
    "        \n",
    "        for item in data:\n",
    "            messages = item['messages']\n",
    "            user_msg = next(msg for msg in messages if msg['role'] == 'user')\n",
    "            assistant_msg = next(msg for msg in messages if msg['role'] == 'assistant')\n",
    "            \n",
    "            instruction_lengths.append(len(user_msg['content']))\n",
    "            response_lengths.append(len(assistant_msg['content']))\n",
    "        \n",
    "        stats = {\n",
    "            'total_samples': total_samples,\n",
    "            'avg_instruction_length': sum(instruction_lengths) / len(instruction_lengths),\n",
    "            'avg_response_length': sum(response_lengths) / len(response_lengths),\n",
    "            'max_instruction_length': max(instruction_lengths),\n",
    "            'max_response_length': max(response_lengths),\n",
    "            'min_instruction_length': min(instruction_lengths),\n",
    "            'min_response_length': min(response_lengths)\n",
    "        }\n",
    "        \n",
    "        return stats\n",
    "    \n",
    "    def run_preprocessing(self):\n",
    "        \"\"\"Main preprocessing pipeline\"\"\"\n",
    "        logger.info(\"Starting data preprocessing...\")\n",
    "        \n",
    "        # Load and process data\n",
    "        df = self.load_data()\n",
    "        processed_data = self.process_data(df)\n",
    "        \n",
    "        # Split data\n",
    "        train_data, val_data, test_data = self.split_data(processed_data)\n",
    "        \n",
    "        # Save datasets\n",
    "        self.save_jsonl(train_data, 'train.jsonl')\n",
    "        self.save_jsonl(val_data, 'validation.jsonl')\n",
    "        self.save_jsonl(test_data, 'test.jsonl')\n",
    "        \n",
    "        # Generate and save statistics\n",
    "        stats = self.generate_statistics(processed_data)\n",
    "        stats_path = os.path.join(self.output_dir, 'dataset_stats.json')\n",
    "        with open(stats_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(stats, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        logger.info(\"Preprocessing completed successfully!\")\n",
    "        logger.info(f\"Dataset statistics: {stats}\")\n",
    "        \n",
    "        return train_data, val_data, test_data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## **Cáº¥u hÃ¬nh Ä‘Æ°á»ng dáº«n & cháº¡y tiá»n xá»­ lÃ½**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T17:17:08.364649Z",
     "iopub.status.busy": "2025-09-17T17:17:08.364304Z",
     "iopub.status.idle": "2025-09-17T17:17:13.370423Z",
     "shell.execute_reply": "2025-09-17T17:17:13.369731Z",
     "shell.execute_reply.started": "2025-09-17T17:17:08.364617Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sá»‘ máº«u Train/Val/Test: 20434 2555 2555\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "# ðŸ‘‰ Sá»¬A Ä‘Æ°á»ng dáº«n nÃ y theo dataset báº¡n Ä‘Ã£ gáº¯n vÃ o notebook\n",
    "# VÃ­ dá»¥ náº¿u dataset name lÃ : hdbank-finetune-data, chá»©a file final_sua_mapped_v2.csv\n",
    "CSV_PATH = \"/kaggle/input/data-banking-processed/final_sua_mapped_v2.csv\"\n",
    "\n",
    "# ThÆ° má»¥c output nÃªn Ä‘á»ƒ trong /kaggle/working\n",
    "OUTPUT_DIR = \"/kaggle/working/data/processed/train_split\"\n",
    "Path(OUTPUT_DIR).mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "preprocessor = BankingDataPreprocessor(CSV_PATH, OUTPUT_DIR)\n",
    "train_data, val_data, test_data = preprocessor.run_preprocessing()\n",
    "\n",
    "print(\"Sá»‘ máº«u Train/Val/Test:\", len(train_data), len(val_data), len(test_data))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ## **Xem nhanh dá»¯ liá»‡u & thá»‘ng kÃª**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T17:17:13.371614Z",
     "iopub.status.busy": "2025-09-17T17:17:13.371299Z",
     "iopub.status.idle": "2025-09-17T17:17:13.380731Z",
     "shell.execute_reply": "2025-09-17T17:17:13.379939Z",
     "shell.execute_reply.started": "2025-09-17T17:17:13.371586Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==> /kaggle/working/data/processed/train_split/train.jsonl\n",
      "{'messages': [{'role': 'system', 'content': 'Báº¡n lÃ  trá»£ lÃ½ tÆ° váº¥n tÃ i chÃ­nh ngÃ¢n hÃ ng chuyÃªn nghiá»‡p cá»§a HDBank. Báº¡n cÃ³ kiáº¿n thá»©c sÃ¢u vá» cÃ¡c sáº£n pháº©m vÃ  dá»‹ch vá»¥ ngÃ¢n hÃ ng, luÃ´n há»— trá»£ khÃ¡ch hÃ ng má»™t cÃ¡ch táº­n tÃ¬nh vÃ  chÃ­nh xÃ¡c. HÃ£y tráº£ lá»i cÃ¡c cÃ¢u há»i má»™t cÃ¡ch chi tiáº¿t, dá»… hiá»ƒu vÃ  thÃ¢n thiá»‡n.'}, {'role': 'user', 'content': 'I cant find my master card will uhlep me lock it (táº¡m dá»‹ch: TÃ´i sáº½ tÃ¬m tháº¥y tháº» chá»§ cá»§a tÃ´i sáº½ uhlep tÃ´i khÃ³a nÃ³)'}, {'role': 'assistant', 'content': 'Xin vui lÃ²ng cháº¥p nháº­n lá»i xin lá»—i cá»§a tÃ´i cho báº¥t ká»³ khÃ³ chá»‹u gÃ¢y ra bá»Ÿi máº¥t tháº» Master cá»§a báº¡n. TÃ´i á»Ÿ Ä‘Ã¢y Ä‘á»ƒ giÃºp báº¡n Ä‘Ã³ng nÃ³ cho sá»± bÃ¬nh an cá»§a báº¡n. Äá»ƒ giÃºp báº¡n vá»›i Ä‘iá»u nÃ y, xin vui lÃ²ng lÃ m theo cÃ¡c bÆ°á»›c sau Ä‘Ã¢y: 1. Kiá»ƒm tra vÃ­ cá»§a báº¡n, tÃºi, hoáº·c báº¥t ká»³ Ä‘á»‹a Ä‘iá»ƒm gáº§n Ä‘Ã¢y báº¡n Ä‘Ã£ truy cáº­p Ä‘á»ƒ xem náº¿u báº¡n cÃ³ thá»ƒ tÃ¬m tháº¥y tháº» Master cá»§a báº¡n. Náº¿u báº¡n tÃ¬m tháº¥y nÃ³, Ä‘áº£m báº£o báº¡n giá»¯ nÃ³ an toÃ n. 2. Náº¿u báº¡n khÃ´ng thá»ƒ tÃ¬m tháº¥y tháº» cá»§a báº¡n, Ä‘iá»u quan trá»ng lÃ  Ä‘á»ƒ bÃ¡o cÃ¡o máº¥t ngay láº­p tá»©c. LiÃªn há»‡ vá»›i nhÃ³m há»— trá»£ khÃ¡ch hÃ ng cá»§a chÃºng tÃ´i táº¡i {{SUPPORT_PHONE}} hoáº·c truy cáº­p trang web cá»§a chÃºng tÃ´i táº¡i {{BILL_PAY_URL}} Ä‘á»ƒ bÃ¡o cÃ¡o tháº» máº¥t. 3. Ä‘áº¡i diá»‡n há»— trá»£ khÃ¡ch hÃ ng cá»§a chÃºng tÃ´i sáº½ hÆ°á»›ng dáº«n báº¡n thÃ´ng qua cÃ¡c bÆ°á»›c cáº§n thiáº¿t Ä‘á»ƒ Ä‘Ã³ng tháº» Master cá»§a báº¡n vÃ  Ä‘áº£m báº£o an toÃ n. Há» cÅ©ng sáº½ giÃºp báº¡n vá»›i báº¥t ká»³ hÆ°á»›ng dáº«n nÃ o khÃ¡c hoáº·c báº¡n cÃ³ thá»ƒ nhá»›.'}]}\n",
      "{'messages': [{'role': 'system', 'content': 'Báº¡n lÃ  trá»£ lÃ½ tÆ° váº¥n tÃ i chÃ­nh ngÃ¢n hÃ ng chuyÃªn nghiá»‡p cá»§a HDBank. Báº¡n cÃ³ kiáº¿n thá»©c sÃ¢u vá» cÃ¡c sáº£n pháº©m vÃ  dá»‹ch vá»¥ ngÃ¢n hÃ ng, luÃ´n há»— trá»£ khÃ¡ch hÃ ng má»™t cÃ¡ch táº­n tÃ¬nh vÃ  chÃ­nh xÃ¡c. HÃ£y tráº£ lá»i cÃ¡c cÃ¢u há»i má»™t cÃ¡ch chi tiáº¿t, dá»… hiá»ƒu vÃ  thÃ¢n thiá»‡n.'}, {'role': 'user', 'content': 'TÃ´i muá»‘n giÃºp Ä‘á»¡ Ä‘á»ƒ tranh luáº­n vá» má»™t giao dá»‹ch ATM'}, {'role': 'assistant', 'content': 'TÃ´i xin lá»—i khi nghe ráº±ng báº¡n Ä‘ang gáº·p má»™t váº¥n Ä‘á» vá»›i tranh cháº¥p má»™t giao dá»‹ch ATM. TÃ´i á»Ÿ Ä‘Ã¢y Ä‘á»ƒ giÃºp báº¡n ra. DÆ°á»›i Ä‘Ã¢y lÃ  cÃ¡c bÆ°á»›c báº¡n cÃ³ thá»ƒ thá»±c hiá»‡n Ä‘á»ƒ tranh cháº¥p giao dá»‹ch: 1. LiÃªn há»‡ vá»›i dá»‹ch vá»¥ khÃ¡ch hÃ ng cá»§a ngÃ¢n hÃ ng cá»§a báº¡n: HÃ£y giáº£i thÃ­ch váº¥n Ä‘á»: Äáº·t cho há» má»™t cuá»™c gá»i táº¡i sá»‘ Ä‘Æ°á»£c cung cáº¥p trÃªn máº·t sau cá»§a tháº» debit / tÃ­n dá»¥ng cá»§a báº¡n hoáº·c trÃªn trang web cá»§a ngÃ¢n hÃ ng cá»§a báº¡n. Há» sáº½ hÆ°á»›ng dáº«n báº¡n thÃ´ng qua quÃ¡ trÃ¬nh tranh cháº¥p giao dá»‹ch. 2. Cung cáº¥p chi tiáº¿t giao dá»‹ch: HÃ£y cháº¯c cháº¯n ráº±ng báº¡n cÃ³ thÃ´ng tin cáº§n thiáº¿t vá» giao dá»‹ch, cháº³ng háº¡n nhÆ° ngÃ y, thá»i gian, vá»‹ trÃ­ vÃ  sá»‘ tiá»n. Äiá»u nÃ y sáº½ giÃºp Ä‘áº¡i diá»‡n dá»‹ch vá»¥ khÃ¡ch hÃ ng giÃºp báº¡n hiá»‡u quáº£ hÆ¡n. 3. Giáº£i thÃ­ch váº¥n Ä‘á»: Báº¡n cÃ³ thá»ƒ giáº£i thÃ­ch rÃµ lÃ½ do táº¡i sao báº¡n tranh cháº¥p giao dá»‹ch ATM. NÃ³ cÃ³ thá»ƒ lÃ  do má»™t giao dá»‹ch khÃ´ng Ä‘Æ°á»£c á»§y quyá»n, má»™t sá»± phÃ¢n biá»‡t vá» sá»‘ tiá»n, hoáº·c báº¥t ká»³ chi tiáº¿t khÃ¡c liÃªn quan Ä‘á»ƒ giáº£i quyáº¿t váº¥n. há» cÃ³ thá»ƒ giÃºp há» trong quÃ¡ trÃ¬nh giáº£i quyáº¿t giao dá»‹ch. 5. náº¿u báº¡n cÃ³ thá»ƒ giÃºp Ä‘á»¡ ngÃ¢n hÃ ng Ä‘á»ƒ giáº£i quyáº¿t váº¥n Ä‘á».'}]}\n",
      "==> /kaggle/working/data/processed/train_split/validation.jsonl\n",
      "{'messages': [{'role': 'system', 'content': 'Báº¡n lÃ  trá»£ lÃ½ tÆ° váº¥n tÃ i chÃ­nh ngÃ¢n hÃ ng chuyÃªn nghiá»‡p cá»§a HDBank. Báº¡n cÃ³ kiáº¿n thá»©c sÃ¢u vá» cÃ¡c sáº£n pháº©m vÃ  dá»‹ch vá»¥ ngÃ¢n hÃ ng, luÃ´n há»— trá»£ khÃ¡ch hÃ ng má»™t cÃ¡ch táº­n tÃ¬nh vÃ  chÃ­nh xÃ¡c. HÃ£y tráº£ lá»i cÃ¡c cÃ¢u há»i má»™t cÃ¡ch chi tiáº¿t, dá»… hiá»ƒu vÃ  thÃ¢n thiá»‡n.'}, {'role': 'user', 'content': 'TÃ´i muá»‘n kiá»ƒm tra thanh toÃ¡n tiá»n á»§ng há»™, báº¡n cÃ³ thá»ƒ giÃºp tÃ´i khÃ´ng?'}, {'role': 'assistant', 'content': 'TÃ´i á»Ÿ Ä‘Ã¢y Ä‘á»ƒ giÃºp báº¡n kiá»ƒm tra thanh toÃ¡n tiá»n áº£o. ChÃºng tÃ´i báº¯t Ä‘áº§u! Äá»ƒ kiá»ƒm tra thanh toÃ¡n tiá»n áº£o cá»§a báº¡n, báº¡n cÃ³ má»™t vÃ i tÃ¹y chá»n: 1. TÃ i khoáº£n trá»±c tuyáº¿n: Náº¿u báº¡n cÃ³ má»™t tÃ i khoáº£n trá»±c tuyáº¿n vá»›i chÃºng tÃ´i, báº¡n cÃ³ thá»ƒ Ä‘Äƒng nháº­p vÃ  duyá»‡t Ä‘áº¿n pháº§n \" tiá»n áº£o\". á»ž Ä‘Ã³, báº¡n sáº½ cÃ³ thá»ƒ xem chi tiáº¿t thanh toÃ¡n tiá»n áº£o cá»§a báº¡n, bao gá»“m sá»‘ tiá»n ná»£, ngÃ y ná»£, vÃ  báº¥t ká»³ cÃ¢n báº±ng xuáº¥t sáº¯c. 2. Há»— trá»£ khÃ¡ch hÃ ng: Náº¿u báº¡n thÃ­ch há»— trá»£ cÃ¡ nhÃ¢n, Ä‘á»™i há»— trá»£ khÃ¡ch hÃ ng chuyÃªn dá»¥ng cá»§a chÃºng tÃ´i cÃ³ sáºµn Ä‘á»ƒ giÃºp báº¡n. Báº¡n cÃ³ thá»ƒ truy cáº­p chÃºng táº¡i {{SUPPORT_PHONE}} trong {{WORKING_HOURS}}. Há» sáº½ hÆ¡n háº¡nh phÃºc cung cáº¥p cho báº¡n thÃ´ng tin báº¡n cáº§n vÃ  tráº£ lá»i báº¥t ká»³ cÃ¢u há»i báº¡n cÃ³ thá»ƒ cÃ³. Náº¿u báº¡n cÃ³ báº¥t ká»³ cÃ¢u há»i hoáº·c cáº§n thÃªm há»— trá»£, hÃ£y cáº£m tháº¥y miá»…n phÃ­ Ä‘á»ƒ biáº¿t tÃ´i.'}]}\n",
      "{'messages': [{'role': 'system', 'content': 'Báº¡n lÃ  trá»£ lÃ½ tÆ° váº¥n tÃ i chÃ­nh ngÃ¢n hÃ ng chuyÃªn nghiá»‡p cá»§a HDBank. Báº¡n cÃ³ kiáº¿n thá»©c sÃ¢u vá» cÃ¡c sáº£n pháº©m vÃ  dá»‹ch vá»¥ ngÃ¢n hÃ ng, luÃ´n há»— trá»£ khÃ¡ch hÃ ng má»™t cÃ¡ch táº­n tÃ¬nh vÃ  chÃ­nh xÃ¡c. HÃ£y tráº£ lá»i cÃ¡c cÃ¢u há»i má»™t cÃ¡ch chi tiáº¿t, dá»… hiá»ƒu vÃ  thÃ¢n thiá»‡n.'}, {'role': 'user', 'content': 'TÃ´i muá»‘n kÃ­ch hoáº¡t má»™t tháº» tÃ­n dá»¥ng trá»±c tuyáº¿n nÆ¡i cÃ³ thá»ƒ ido nÃ³'}, {'role': 'assistant', 'content': 'Cháº¯c cháº¯n! tÃ´i á»Ÿ Ä‘Ã¢y Ä‘á»ƒ giÃºp báº¡n kÃ­ch hoáº¡t tháº» tÃ­n dá»¥ng cá»§a báº¡n trá»±c tuyáº¿n. Ä‘á»ƒ kÃ­ch hoáº¡t tháº» tÃ­n dá»¥ng cá»§a báº¡n, vui lÃ²ng lÃ m theo cÃ¡c bÆ°á»›c sau Ä‘Ã¢y: 1. truy cáº­p trang web chÃ­nh thá»©c cá»§a nhÃ  cung cáº¥p tháº» tÃ­n dá»¥ng cá»§a báº¡n. 2. tÃ¬m kiáº¿m táº¥t cáº£ cÃ¡c chi tiáº¿t cáº§n thiáº¿t, báº¥m vÃ o pháº§n \"Activate Card\" hoáº·c \"Submit\" nÃºt. NÃ³ thÆ°á»ng náº±m trong \"Account\" hoáº·c \"Online Services\" khu vá»±c cá»§a trang web. 3. báº¥m vÃ o tÃ¹y chá»n \"Activate Card\" 4. báº¡n cÃ³ thá»ƒ Ä‘Æ°á»£c yÃªu cáº§u cung cáº¥p má»™t sá»‘ thÃ´ng tin nhÆ° sá»‘ tháº» cá»§a báº¡n, tÃªn, ngÃ y háº¿t háº¡n, vÃ  mÃ£ báº£o máº­t. HÃ£y cháº¯c cháº¯n ráº±ng tháº» cá»§a báº¡n cÃ³ sáºµn. 5. cáº©n tháº­n nháº­p thÃ´ng tin cáº§n thiáº¿t vÃ  lÃ m theo báº¥t ká»³ hÆ°á»›ng dáº«n bá»• sung Ä‘Æ°á»£c cung cáº¥p trÃªn trang web. 6. Sau khi báº¡n Ä‘Ã£ nháº­p táº¥t cáº£ cÃ¡c chi tiáº¿t cáº§n thiáº¿t, báº¥m vÃ o \"Activate Card\" hoáº·c \"Submit\" nÃºt. 7. Sau khi kÃ­ch hoáº¡t tháº» cá»§a báº¡n, báº¡n cÃ³ thá»ƒ Ä‘Æ°á»£c khuyáº¿n khÃ­ch Ä‘á»ƒ táº¡o má»™t tÃ i khoáº£n trá»±c tuyáº¿n cho sá»‘ tháº» tÃ­n dá»¥ng hoáº·c tháº» tÃ­n dá»¥ng cá»§a báº¡n.'}]}\n",
      "==> /kaggle/working/data/processed/train_split/test.jsonl\n",
      "{'messages': [{'role': 'system', 'content': 'Báº¡n lÃ  trá»£ lÃ½ tÆ° váº¥n tÃ i chÃ­nh ngÃ¢n hÃ ng chuyÃªn nghiá»‡p cá»§a HDBank. Báº¡n cÃ³ kiáº¿n thá»©c sÃ¢u vá» cÃ¡c sáº£n pháº©m vÃ  dá»‹ch vá»¥ ngÃ¢n hÃ ng, luÃ´n há»— trá»£ khÃ¡ch hÃ ng má»™t cÃ¡ch táº­n tÃ¬nh vÃ  chÃ­nh xÃ¡c. HÃ£y tráº£ lá»i cÃ¡c cÃ¢u há»i má»™t cÃ¡ch chi tiáº¿t, dá»… hiá»ƒu vÃ  thÃ¢n thiá»‡n.'}, {'role': 'user', 'content': 'TÃ´i muá»‘n há»§y khoáº£n vay, giÃºp tÃ´i'}, {'role': 'assistant', 'content': 'TÃ´i xin lá»—i khi nghe ráº±ng báº¡n Ä‘ang tÃ¬m cÃ¡ch há»§y khoáº£n vay cá»§a báº¡n. tÃ´i hiá»ƒu ráº±ng hoÃ n cáº£nh cÃ³ thá»ƒ thay Ä‘á»•i, vÃ¬ váº­y tÃ´i sáº½ lÃ m tá»‘t nháº¥t Ä‘á»ƒ giÃºp báº¡n vá»›i quÃ¡ trÃ¬nh há»§y khoáº£n vay. Äá»ƒ há»§y khoáº£n vay cá»§a báº¡n, vui lÃ²ng lÃ m theo cÃ¡c bÆ°á»›c sau Ä‘Ã¢y: 1. LiÃªn há»‡ vá»›i nhÃ³m dá»‹ch vá»¥ khÃ¡ch hÃ ng cá»§a chÃºng tÃ´i táº¡i {{SUPPORT_PHONE}} hoáº·c gá»­i email Ä‘áº¿n {{SUPPORT_EMAIL}} vá»›i yÃªu cáº§u cá»§a báº¡n. há» sáº½ hÆ°á»›ng dáº«n báº¡n thÃ´ng qua quÃ¡ trÃ¬nh há»§y khoáº£n vay vÃ  cung cáº¥p báº¥t ká»³ hÃ¬nh thá»©c hoáº·c tÃ i liá»‡u cáº§n thiáº¿t. 2. Cung cáº¥p chi tiáº¿t khoáº£n vay cá»§a báº¡n, cháº³ng háº¡n nhÆ° sá»‘ khoáº£n vay, sá»‘ khoáº£n vay, vÃ  báº¥t ká»³ thÃ´ng tin liÃªn quan khÃ¡c, cho Ä‘áº¡i diá»‡n dá»‹ch vá»¥ khÃ¡ch hÃ ng. Äiá»u nÃ y sáº½ giÃºp tÄƒng tá»‘c quÃ¡ trÃ¬nh há»§y khoáº£n vay. 3. Náº¿u cÃ³ báº¥t ká»³ cÃ¢n báº±ng hoáº·c phÃ­ liÃªn quan Ä‘áº¿n khoáº£n vay cá»§a báº¡n, hÃ£y cháº¯c cháº¯n Ä‘á»ƒ tháº£o luáº­n vá»›i Ä‘áº¡i diá»‡n dá»‹ch vá»¥ khÃ¡ch hÃ ng.'}]}\n",
      "{'messages': [{'role': 'system', 'content': 'Báº¡n lÃ  trá»£ lÃ½ tÆ° váº¥n tÃ i chÃ­nh ngÃ¢n hÃ ng chuyÃªn nghiá»‡p cá»§a HDBank. Báº¡n cÃ³ kiáº¿n thá»©c sÃ¢u vá» cÃ¡c sáº£n pháº©m vÃ  dá»‹ch vá»¥ ngÃ¢n hÃ ng, luÃ´n há»— trá»£ khÃ¡ch hÃ ng má»™t cÃ¡ch táº­n tÃ¬nh vÃ  chÃ­nh xÃ¡c. HÃ£y tráº£ lá»i cÃ¡c cÃ¢u há»i má»™t cÃ¡ch chi tiáº¿t, dá»… hiá»ƒu vÃ  thÃ¢n thiá»‡n.'}, {'role': 'user', 'content': 'TÃ´i pháº£i chuyá»ƒn, tÃ´i cáº§n sá»± giÃºp Ä‘á»¡'}, {'role': 'assistant', 'content': 'TÃ´i á»Ÿ Ä‘Ã¢y Ä‘á»ƒ giÃºp báº¡n thá»±c hiá»‡n má»™t chuyá»ƒn khoáº£n. Äá»ƒ Ä‘áº£m báº£o má»™t quÃ¡ trÃ¬nh thuáº­n tiá»‡n, vui lÃ²ng lÃ m theo cÃ¡c bÆ°á»›c sau Ä‘Ã¢y: 1. ÄÄƒng nháº­p vÃ o tÃ i khoáº£n ngÃ¢n hÃ ng trá»±c tuyáº¿n hoáº·c á»©ng dá»¥ng ngÃ¢n hÃ ng di Ä‘á»™ng cá»§a báº¡n. 2. Truy cáº­p vÃ o pháº§n \"TÃ i khoáº£n\" hoáº·c \" tiá»n\" pháº§n. 3. Chá»n tÃ¹y chá»n Ä‘á»ƒ thá»±c hiá»‡n má»™t chuyá»ƒn khoáº£n má»›i Ä‘áº¿n tÃ i khoáº£n ngÃ¢n hÃ ng khÃ¡c. 4. Cung cáº¥p cho tÃ i khoáº£n ngÃ¢n hÃ ng cá»§a ngÆ°á»i nháº­n thÃ´ng tin, cháº³ng háº¡n nhÆ° sá»‘ tÃ i khoáº£n vÃ  sá»‘ Ä‘á»‹nh tuyáº¿n. ÄÆ¡n giáº£n kiá»ƒm tra thÃ´ng tin Ä‘á»ƒ xÃ¡c Ä‘á»‹nh chÃ­nh xÃ¡c. 5. Chá»n sá»‘ khoáº£n chuyá»ƒn khoáº£n vÃ  chá»n tÃ i khoáº£n tá»« Ä‘Ã³ báº¡n muá»‘n gá»­i tiá»n. 6. Náº¿u Ä‘Æ°á»£c thÃºc Ä‘áº©y, thÃªm má»™t tham chiáº¿u hoáº·c ghi chÃº cho chuyá»ƒn khoáº£n. 7. Xem chi tiáº¿t ká»¹ lÆ°á»¡ng vÃ  Ä‘áº£m báº£o má»i thá»© lÃ  chÃ­nh xÃ¡c. 8. Cuá»‘i cÃ¹ng, xÃ¡c nháº­n chuyá»ƒn khoáº£n Ä‘á»ƒ khá»Ÿi Ä‘á»™ng. Náº¿u báº¡n gáº·p báº¥t ká»³ khÃ³ khÄƒn hoáº·c cÃ³ cÃ¢u há»i cá»¥ thá»ƒ trong quÃ¡ trÃ¬nh, cáº£m tháº¥y miá»…n phÃ­ Ä‘á»ƒ truy cáº­p vÃ o sá»‘ Ä‘iá»‡n thoáº¡i.'}]}\n",
      "\n",
      "==> dataset_stats.json\n",
      "{'total_samples': 25544, 'avg_instruction_length': 60.644808957093645, 'avg_response_length': 751.8644691512684, 'max_instruction_length': 2044, 'max_response_length': 3481, 'min_instruction_length': 7, 'min_response_length': 76}\n"
     ]
    }
   ],
   "source": [
    "import json, os, itertools\n",
    "\n",
    "def peek_jsonl(path, n=3):\n",
    "    print(f\"==> {path}\")\n",
    "    with open(path, 'r', encoding='utf-8') as f:\n",
    "        for i, line in zip(range(n), f):\n",
    "            print(json.loads(line))\n",
    "\n",
    "base = OUTPUT_DIR\n",
    "for name in [\"train.jsonl\", \"validation.jsonl\", \"test.jsonl\"]:\n",
    "    peek_jsonl(os.path.join(base, name), n=2)\n",
    "\n",
    "print(\"\\n==> dataset_stats.json\")\n",
    "with open(os.path.join(base, \"dataset_stats.json\"), \"r\", encoding=\"utf-8\") as f:\n",
    "    print(json.load(f))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "4c94a421-d46b-471c-b670-06549cd80cb7",
    "_uuid": "aeea72f3-9375-4523-9262-ec07341a82ae",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Session 2 â€” LoRA Configuration\n",
    "Import nguyÃªn `lora_config.py` vÃ  in tÃ³m táº¯t cáº¥u hÃ¬nh + Æ°á»›c lÆ°á»£ng bá»™ nhá»›."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "b0f1853e-7957-4219-8878-98e3a98f0a0e",
    "_uuid": "60c4df52-5258-42c3-915e-0385b6466ae2",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-17T17:17:13.382180Z",
     "iopub.status.busy": "2025-09-17T17:17:13.381868Z",
     "iopub.status.idle": "2025-09-17T17:17:13.404496Z",
     "shell.execute_reply": "2025-09-17T17:17:13.403586Z",
     "shell.execute_reply.started": "2025-09-17T17:17:13.382150Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "LoRA Configuration for Qwen3-4B Banking Chatbot Fine-tuning\n",
    "Cáº¥u hÃ¬nh LoRA Ä‘Æ°á»£c tá»‘i Æ°u cho VRAM vÃ  cháº¥t lÆ°á»£ng training\n",
    "\"\"\"\n",
    "\n",
    "from peft import LoraConfig, TaskType\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Optional\n",
    "import torch\n",
    "\n",
    "@dataclass\n",
    "class LoRAConfigManager:\n",
    "    \"\"\"\n",
    "    LoRA Configuration Manager vá»›i cÃ¡c tham sá»‘ Ä‘Æ°á»£c tá»‘i Æ°u cho Qwen3-4B\n",
    "    \n",
    "    PhÃ¢n tÃ­ch tham sá»‘:\n",
    "    - r=16: Rank tháº¥p vá»«a Ä‘á»§ Ä‘á»ƒ capture patterns quan trá»ng, tiáº¿t kiá»‡m VRAM\n",
    "    - lora_alpha=32: Scaling factor = 2*r, cÃ¢n báº±ng tá»‘t giá»¯a stability vÃ  learning capacity\n",
    "    - target_modules: Táº­p trung vÃ o attention vÃ  MLP layers quan trá»ng nháº¥t\n",
    "    - lora_dropout=0.1: Regularization vá»«a pháº£i, trÃ¡nh overfitting\n",
    "    \"\"\"\n",
    "    \n",
    "    # Core LoRA parameters\n",
    "    r: int = 16                    # Rank - cÃ¢n báº±ng hiá»‡u suáº¥t/cháº¥t lÆ°á»£ng\n",
    "    lora_alpha: int = 32           # Scaling factor (2*r)\n",
    "    lora_dropout: float = 0.1      # Dropout cho LoRA layers\n",
    "    bias: str = \"none\"             # KhÃ´ng train bias Ä‘á»ƒ tiáº¿t kiá»‡m memory\n",
    "    \n",
    "    # Target modules cho Qwen3-4B architecture\n",
    "    target_modules: List[str] = None\n",
    "    \n",
    "    # Task configuration\n",
    "    task_type: TaskType = TaskType.CAUSAL_LM\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Initialize target modules if not provided\"\"\"\n",
    "        if self.target_modules is None:\n",
    "            self.target_modules = [\n",
    "                # Attention layers - quan trá»ng nháº¥t cho language understanding\n",
    "                \"q_proj\",      # Query projection\n",
    "                \"k_proj\",      # Key projection  \n",
    "                \"v_proj\",      # Value projection\n",
    "                \"o_proj\",      # Output projection\n",
    "                \n",
    "                # MLP layers - quan trá»ng cho knowledge representation\n",
    "                \"gate_proj\",   # Gate projection in MLP\n",
    "                \"up_proj\",     # Up projection in MLP\n",
    "                \"down_proj\",   # Down projection in MLP\n",
    "            ]\n",
    "    \n",
    "    def get_lora_config(self) -> LoraConfig:\n",
    "        \"\"\"Create LoRA configuration\"\"\"\n",
    "        return LoraConfig(\n",
    "            r=self.r,\n",
    "            lora_alpha=self.lora_alpha,\n",
    "            target_modules=self.target_modules,\n",
    "            lora_dropout=self.lora_dropout,\n",
    "            bias=self.bias,\n",
    "            task_type=self.task_type,\n",
    "            inference_mode=False,\n",
    "        )\n",
    "    \n",
    "    def get_memory_efficient_config(self) -> LoraConfig:\n",
    "        \"\"\"Get memory-efficient LoRA config for limited VRAM\"\"\"\n",
    "        return LoraConfig(\n",
    "            r=8,                    # Giáº£m rank Ä‘á»ƒ tiáº¿t kiá»‡m memory\n",
    "            lora_alpha=16,          # TÆ°Æ¡ng á»©ng giáº£m alpha\n",
    "            target_modules=[\n",
    "                \"q_proj\", \"v_proj\",  # Chá»‰ train query vÃ  value\n",
    "                \"down_proj\"          # VÃ  output MLP\n",
    "            ],\n",
    "            lora_dropout=0.1,\n",
    "            bias=\"none\",\n",
    "            task_type=self.task_type,\n",
    "            inference_mode=False,\n",
    "        )\n",
    "    \n",
    "    def get_high_quality_config(self) -> LoraConfig:\n",
    "        \"\"\"Get high-quality LoRA config for better performance\"\"\"\n",
    "        return LoraConfig(\n",
    "            r=32,                   # Rank cao hÆ¡n cho cháº¥t lÆ°á»£ng tá»‘t hÆ¡n\n",
    "            lora_alpha=64,          # Alpha tÆ°Æ¡ng á»©ng\n",
    "            target_modules=self.target_modules + [\n",
    "                \"embed_tokens\",     # ThÃªm embedding layer\n",
    "                \"lm_head\"          # ThÃªm language model head\n",
    "            ],\n",
    "            lora_dropout=0.05,      # Dropout tháº¥p hÆ¡n\n",
    "            bias=\"none\",\n",
    "            task_type=self.task_type,\n",
    "            inference_mode=False,\n",
    "        )\n",
    "    \n",
    "    def estimate_memory_usage(self, model_size_gb: float = 8.0) -> dict:\n",
    "        \"\"\"Estimate memory usage for different configurations\"\"\"\n",
    "        \n",
    "        # Base model memory\n",
    "        base_memory = model_size_gb\n",
    "        \n",
    "        # LoRA parameters estimation\n",
    "        # Rough calculation: r * (input_dim + output_dim) * num_layers * num_target_modules\n",
    "        qwen_hidden_size = 3584  # Qwen3-4B hidden size\n",
    "        num_layers = 40          # Qwen3-4B layers\n",
    "        num_target_modules = len(self.target_modules)\n",
    "        \n",
    "        # Standard config\n",
    "        standard_params = self.r * qwen_hidden_size * num_layers * num_target_modules\n",
    "        standard_memory = standard_params * 4 / (1024**3)  # 4 bytes per param, convert to GB\n",
    "        \n",
    "        # Memory efficient config  \n",
    "        efficient_params = 8 * qwen_hidden_size * num_layers * 3  # r=8, 3 modules\n",
    "        efficient_memory = efficient_params * 4 / (1024**3)\n",
    "        \n",
    "        # High quality config\n",
    "        quality_params = 32 * qwen_hidden_size * num_layers * (num_target_modules + 2)\n",
    "        quality_memory = quality_params * 4 / (1024**3)\n",
    "        \n",
    "        return {\n",
    "            \"base_model_memory_gb\": base_memory,\n",
    "            \"standard_lora_memory_gb\": standard_memory,\n",
    "            \"efficient_lora_memory_gb\": efficient_memory,\n",
    "            \"quality_lora_memory_gb\": quality_memory,\n",
    "            \"total_standard_gb\": base_memory + standard_memory,\n",
    "            \"total_efficient_gb\": base_memory + efficient_memory,\n",
    "            \"total_quality_gb\": base_memory + quality_memory\n",
    "        }\n",
    "    \n",
    "    def print_config_summary(self):\n",
    "        \"\"\"Print configuration summary\"\"\"\n",
    "        print(\"=== LoRA Configuration Summary ===\")\n",
    "        print(f\"Rank (r): {self.r}\")\n",
    "        print(f\"Alpha: {self.lora_alpha}\")\n",
    "        print(f\"Dropout: {self.lora_dropout}\")\n",
    "        print(f\"Target modules: {len(self.target_modules)}\")\n",
    "        print(f\"Modules: {', '.join(self.target_modules)}\")\n",
    "        \n",
    "        memory_info = self.estimate_memory_usage()\n",
    "        print(f\"\\n=== Memory Estimation ===\")\n",
    "        print(f\"Standard config total: {memory_info['total_standard_gb']:.2f} GB\")\n",
    "        print(f\"Efficient config total: {memory_info['total_efficient_gb']:.2f} GB\") \n",
    "        print(f\"Quality config total: {memory_info['total_quality_gb']:.2f} GB\")\n",
    "\n",
    "def get_recommended_config(available_vram_gb: float) -> LoraConfig:\n",
    "    \"\"\"Get recommended LoRA config based on available VRAM\"\"\"\n",
    "    config_manager = LoRAConfigManager()\n",
    "    \n",
    "    if available_vram_gb >= 16:\n",
    "        print(\"Using high-quality LoRA configuration\")\n",
    "        return config_manager.get_high_quality_config()\n",
    "    elif available_vram_gb >= 12:\n",
    "        print(\"Using standard LoRA configuration\")\n",
    "        return config_manager.get_lora_config()\n",
    "    else:\n",
    "        print(\"Using memory-efficient LoRA configuration\")\n",
    "        return config_manager.get_memory_efficient_config()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "5fcf2590-3de3-4862-a2fb-e3010a17262f",
    "_uuid": "78f0ebcd-8036-41e7-83ec-309c4a75db2a",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Session 3 â€” Training Configuration\n",
    "Import nguyÃªn `training_config.py`, in tÃ³m táº¯t cáº¥u hÃ¬nh vÃ  lá»‹ch train Æ°á»›c tÃ­nh dá»±a trÃªn sá»‘ máº«u Ä‘Ã£ tiá»n xá»­ lÃ½."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "c4e1f5ca-361b-4d3b-903a-3e034ea4f6b1",
    "_uuid": "4720a8a7-dd31-40c2-b00d-7564e72a6156",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-17T17:17:13.410913Z",
     "iopub.status.busy": "2025-09-17T17:17:13.410194Z",
     "iopub.status.idle": "2025-09-17T17:17:13.489469Z",
     "shell.execute_reply": "2025-09-17T17:17:13.488592Z",
     "shell.execute_reply.started": "2025-09-17T17:17:13.410887Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Training Configuration for Qwen2.5-4B Banking Chatbot Fine-tuning\n",
    "Cáº¥u hÃ¬nh training Ä‘Æ°á»£c tá»‘i Æ°u cho cháº¥t lÆ°á»£ng vÃ  hiá»‡u suáº¥t\n",
    "\"\"\"\n",
    "\n",
    "from transformers import TrainingArguments\n",
    "from dataclasses import dataclass\n",
    "from typing import Optional\n",
    "import torch\n",
    "import os\n",
    "\n",
    "@dataclass\n",
    "class TrainingConfigManager:\n",
    "    \"\"\"\n",
    "    Training Configuration Manager vá»›i cÃ¡c tham sá»‘ Ä‘Æ°á»£c nghiÃªn cá»©u ká»¹ lÆ°á»¡ng\n",
    "    \n",
    "    PhÃ¢n tÃ­ch tham sá»‘:\n",
    "    - learning_rate=2e-4: Optimal cho LoRA fine-tuning, khÃ´ng quÃ¡ cao gÃ¢y instability\n",
    "    - batch_size=4: CÃ¢n báº±ng giá»¯a memory usage vÃ  gradient stability\n",
    "    - gradient_accumulation=4: Effective batch size = 16, Ä‘á»§ lá»›n cho stable training\n",
    "    - warmup_steps=100: Warm-up Ä‘á»ƒ trÃ¡nh gradient explosion á»Ÿ Ä‘áº§u training\n",
    "    - max_steps: ÄÆ°á»£c tÃ­nh dá»±a trÃªn dataset size vÃ  epochs\n",
    "    \"\"\"\n",
    "    \n",
    "    # Core training parameters\n",
    "    output_dir: str = \"./qwen-banking-lora\"\n",
    "    num_train_epochs: int = 3\n",
    "    per_device_train_batch_size: int = 4\n",
    "    per_device_eval_batch_size: int = 4\n",
    "    gradient_accumulation_steps: int = 4\n",
    "    \n",
    "    # Learning rate and optimization\n",
    "    learning_rate: float = 2e-4\n",
    "    weight_decay: float = 0.01\n",
    "    warmup_steps: int = 100\n",
    "    lr_scheduler_type: str = \"cosine\"\n",
    "    \n",
    "    # Mixed precision and optimization\n",
    "    fp16: bool = True\n",
    "    bf16: bool = False  # Set to True if using Ampere+ GPUs\n",
    "    gradient_checkpointing: bool = True\n",
    "    dataloader_pin_memory: bool = False\n",
    "    \n",
    "    # Evaluation and logging\n",
    "    eval_strategy: str = \"steps\"\n",
    "    eval_steps: int = 100\n",
    "    logging_steps: int = 10\n",
    "    save_strategy: str = \"steps\"\n",
    "    save_steps: int = 200\n",
    "    save_total_limit: int = 3\n",
    "    \n",
    "    # Early stopping and best model\n",
    "    load_best_model_at_end: bool = True\n",
    "    metric_for_best_model: str = \"eval_loss\"\n",
    "    greater_is_better: bool = False\n",
    "    \n",
    "    # Memory optimization\n",
    "    remove_unused_columns: bool = False\n",
    "    dataloader_num_workers: int = 0\n",
    "    \n",
    "    def __post_init__(self):\n",
    "        \"\"\"Post-initialization setup\"\"\"\n",
    "        # Auto-detect bf16 support\n",
    "        if torch.cuda.is_available():\n",
    "            device_capability = torch.cuda.get_device_capability()\n",
    "            if device_capability[0] >= 8:  # Ampere or newer\n",
    "                self.bf16 = True\n",
    "                self.fp16 = False\n",
    "                print(\"Using BF16 precision (Ampere+ GPU detected)\")\n",
    "            else:\n",
    "                print(\"Using FP16 precision\")\n",
    "        \n",
    "        # Create output directory\n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "    \n",
    "    def get_training_args(self, max_steps: Optional[int] = None) -> TrainingArguments:\n",
    "        \"\"\"Create TrainingArguments object\"\"\"\n",
    "        \n",
    "        args_dict = {\n",
    "            \"output_dir\": self.output_dir,\n",
    "            \"num_train_epochs\": self.num_train_epochs,\n",
    "            \"per_device_train_batch_size\": self.per_device_train_batch_size,\n",
    "            \"per_device_eval_batch_size\": self.per_device_eval_batch_size,\n",
    "            \"gradient_accumulation_steps\": self.gradient_accumulation_steps,\n",
    "            \"learning_rate\": self.learning_rate,\n",
    "            \"weight_decay\": self.weight_decay,\n",
    "            \"warmup_steps\": self.warmup_steps,\n",
    "            \"lr_scheduler_type\": self.lr_scheduler_type,\n",
    "            \"fp16\": self.fp16,\n",
    "            \"bf16\": self.bf16,\n",
    "            \"gradient_checkpointing\": self.gradient_checkpointing,\n",
    "            \"dataloader_pin_memory\": self.dataloader_pin_memory,\n",
    "            \"eval_strategy\": self.eval_strategy,\n",
    "            \"eval_steps\": self.eval_steps,\n",
    "            \"logging_steps\": self.logging_steps,\n",
    "            \"save_strategy\": self.save_strategy,\n",
    "            \"save_steps\": self.save_steps,\n",
    "            \"save_total_limit\": self.save_total_limit,\n",
    "            \"load_best_model_at_end\": self.load_best_model_at_end,\n",
    "            \"metric_for_best_model\": self.metric_for_best_model,\n",
    "            \"greater_is_better\": self.greater_is_better,\n",
    "            \"remove_unused_columns\": self.remove_unused_columns,\n",
    "            \"dataloader_num_workers\": self.dataloader_num_workers,\n",
    "            \"report_to\": \"none\",  # Disable wandb/tensorboard by default\n",
    "            \"seed\": 42,\n",
    "        }\n",
    "        \n",
    "        if max_steps is not None:\n",
    "            args_dict[\"max_steps\"] = max_steps\n",
    "            args_dict.pop(\"num_train_epochs\")  # Remove epochs if using max_steps\n",
    "        \n",
    "        return TrainingArguments(**args_dict)\n",
    "    \n",
    "    def get_memory_efficient_args(self, max_steps: Optional[int] = None) -> TrainingArguments:\n",
    "        \"\"\"Get memory-efficient training arguments\"\"\"\n",
    "        # Reduce batch sizes for memory efficiency\n",
    "        self.per_device_train_batch_size = 2\n",
    "        self.per_device_eval_batch_size = 2\n",
    "        self.gradient_accumulation_steps = 8  # Keep effective batch size = 16\n",
    "        self.gradient_checkpointing = True\n",
    "        self.dataloader_pin_memory = False\n",
    "        \n",
    "        return self.get_training_args(max_steps)\n",
    "    \n",
    "    def get_high_performance_args(self, max_steps: Optional[int] = None) -> TrainingArguments:\n",
    "        \"\"\"Get high-performance training arguments\"\"\"\n",
    "        # Increase batch sizes for better performance\n",
    "        self.per_device_train_batch_size = 4\n",
    "        self.per_device_eval_batch_size = 4\n",
    "        self.gradient_accumulation_steps = 8  # Effective batch size = 16\n",
    "        self.dataloader_num_workers = 4\n",
    "        self.dataloader_pin_memory = True\n",
    "        \n",
    "        return self.get_training_args(max_steps)\n",
    "    \n",
    "    def calculate_training_steps(self, dataset_size: int) -> dict:\n",
    "        \"\"\"Calculate training steps and duration\"\"\"\n",
    "        effective_batch_size = (\n",
    "            self.per_device_train_batch_size * \n",
    "            self.gradient_accumulation_steps\n",
    "        )\n",
    "        \n",
    "        steps_per_epoch = dataset_size // effective_batch_size\n",
    "        total_steps = steps_per_epoch * self.num_train_epochs\n",
    "        \n",
    "        # Estimate training time (rough approximation)\n",
    "        # Assuming ~1.5 seconds per step on RTX 4090\n",
    "        estimated_time_hours = (total_steps * 1.5) / 3600\n",
    "        \n",
    "        return {\n",
    "            \"dataset_size\": dataset_size,\n",
    "            \"effective_batch_size\": effective_batch_size,\n",
    "            \"steps_per_epoch\": steps_per_epoch,\n",
    "            \"total_training_steps\": total_steps,\n",
    "            \"estimated_time_hours\": estimated_time_hours,\n",
    "            \"warmup_ratio\": self.warmup_steps / total_steps if total_steps > 0 else 0\n",
    "        }\n",
    "    \n",
    "    def print_config_summary(self, dataset_size: Optional[int] = None):\n",
    "        \"\"\"Print training configuration summary\"\"\"\n",
    "        print(\"=== Training Configuration Summary ===\")\n",
    "        print(f\"Learning rate: {self.learning_rate}\")\n",
    "        print(f\"Batch size per device: {self.per_device_train_batch_size}\")\n",
    "        print(f\"Gradient accumulation: {self.gradient_accumulation_steps}\")\n",
    "        print(f\"Effective batch size: {self.per_device_train_batch_size * self.gradient_accumulation_steps}\")\n",
    "        print(f\"Epochs: {self.num_train_epochs}\")\n",
    "        print(f\"Warmup steps: {self.warmup_steps}\")\n",
    "        print(f\"Weight decay: {self.weight_decay}\")\n",
    "        print(f\"Precision: {'BF16' if self.bf16 else 'FP16' if self.fp16 else 'FP32'}\")\n",
    "        print(f\"Gradient checkpointing: {self.gradient_checkpointing}\")\n",
    "        \n",
    "        if dataset_size:\n",
    "            training_info = self.calculate_training_steps(dataset_size)\n",
    "            print(f\"\\n=== Training Schedule ===\")\n",
    "            print(f\"Dataset size: {training_info['dataset_size']}\")\n",
    "            print(f\"Steps per epoch: {training_info['steps_per_epoch']}\")\n",
    "            print(f\"Total training steps: {training_info['total_training_steps']}\")\n",
    "            print(f\"Estimated time: {training_info['estimated_time_hours']:.1f} hours\")\n",
    "            print(f\"Warmup ratio: {training_info['warmup_ratio']:.3f}\")\n",
    "\n",
    "def get_recommended_training_config(available_vram_gb: float, dataset_size: int) -> TrainingArguments:\n",
    "    \"\"\"Get recommended training config based on available VRAM\"\"\"\n",
    "    config_manager = TrainingConfigManager()\n",
    "    \n",
    "    # Calculate max steps for better control\n",
    "    training_info = config_manager.calculate_training_steps(dataset_size)\n",
    "    max_steps = training_info['total_training_steps']\n",
    "    \n",
    "    if available_vram_gb >= 16:\n",
    "        print(\"Using high-performance training configuration\")\n",
    "        return config_manager.get_high_performance_args(max_steps)\n",
    "    elif available_vram_gb >= 12:\n",
    "        print(\"Using standard training configuration\")\n",
    "        return config_manager.get_training_args(max_steps)\n",
    "    else:\n",
    "        print(\"Using memory-efficient training configuration\")\n",
    "        return config_manager.get_memory_efficient_args(max_steps)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compat + Memory-Efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T17:17:13.490827Z",
     "iopub.status.busy": "2025-09-17T17:17:13.490487Z",
     "iopub.status.idle": "2025-09-17T17:17:13.536226Z",
     "shell.execute_reply": "2025-09-17T17:17:13.535300Z",
     "shell.execute_reply.started": "2025-09-17T17:17:13.490796Z"
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Detected dataset_size = 20434\n",
      "Available VRAM â‰ˆ 17.1 GB\n",
      "Using FP16 precision\n",
      "=== Training Configuration Summary ===\n",
      "Learning rate: 0.0002\n",
      "Batch size per device: 4\n",
      "Gradient accumulation: 4\n",
      "Effective batch size: 16\n",
      "Epochs: 3\n",
      "Warmup steps: 100\n",
      "Weight decay: 0.01\n",
      "Precision: FP16\n",
      "Gradient checkpointing: True\n",
      "\n",
      "=== Training Schedule ===\n",
      "Dataset size: 20434\n",
      "Steps per epoch: 1277\n",
      "Total training steps: 3831\n",
      "Estimated time: 1.6 hours\n",
      "Warmup ratio: 0.026\n",
      "\n",
      "== Preview key TrainingArguments ==\n",
      "output_dir: ./qwen-banking-lora\n",
      "per_device_train_batch_size: 1\n",
      "gradient_accumulation_steps: 16\n",
      "learning_rate: 0.0002\n",
      "eval/log/save steps: 100 10 200\n",
      "precision fp16/bf16: True False\n",
      "optim: OptimizerNames.PAGED_ADAMW_8BIT\n"
     ]
    }
   ],
   "source": [
    "# === Compat + Memory-Efficient TrainingArguments builder ===\n",
    "import os, json, torch\n",
    "from dataclasses import fields as dataclass_fields\n",
    "from transformers import TrainingArguments as HFTrainingArguments\n",
    "\n",
    "# 1) dataset_size\n",
    "if 'train_data' in globals():\n",
    "    dataset_size = len(train_data)\n",
    "else:\n",
    "    dataset_size = 0\n",
    "    guess = \"/kaggle/working/data/processed/train_split/train.jsonl\"\n",
    "    if os.path.exists(guess):\n",
    "        with open(guess, \"r\", encoding=\"utf-8\") as f:\n",
    "            for _ in f:\n",
    "                dataset_size += 1\n",
    "    else:\n",
    "        dataset_size = 1000\n",
    "print(f\"Detected dataset_size = {dataset_size}\")\n",
    "\n",
    "# 2) VRAM\n",
    "available_vram = 8.0\n",
    "if torch.cuda.is_available():\n",
    "    props = torch.cuda.get_device_properties(0)\n",
    "    available_vram = props.total_memory / 1e9\n",
    "print(f\"Available VRAM â‰ˆ {available_vram:.1f} GB\")\n",
    "\n",
    "# 3) config summary\n",
    "tcm = TrainingConfigManager()\n",
    "tcm.print_config_summary(dataset_size=dataset_size)\n",
    "\n",
    "# 4) Ã‰P profile memory-efficient (trÃ¡nh BS=8)\n",
    "#    Giá»¯ Ä‘Ãºng logic function get_memory_efficient_args cá»§a báº¡n:\n",
    "tcm.per_device_train_batch_size = 1    # máº¡nh tay Ä‘á»ƒ cháº¯c cháº¯n fit\n",
    "tcm.per_device_eval_batch_size = 1\n",
    "tcm.gradient_accumulation_steps = 16   # effective batch váº«n 16\n",
    "tcm.gradient_checkpointing = True\n",
    "tcm.dataloader_pin_memory = False\n",
    "\n",
    "# 5) DÃ¹ng AdamW 8-bit (giáº£m RAM) náº¿u transformers há»— trá»£ qua TrainingArguments\n",
    "extra_optim = {\"optim\": \"paged_adamw_8bit\"}  # cáº§n bitsandbytes\n",
    "\n",
    "# 6) Build args_dict giá»‘ng há»‡t get_training_args\n",
    "args_dict = {\n",
    "    \"output_dir\": tcm.output_dir,\n",
    "    \"num_train_epochs\": tcm.num_train_epochs,\n",
    "    \"per_device_train_batch_size\": tcm.per_device_train_batch_size,\n",
    "    \"per_device_eval_batch_size\": tcm.per_device_eval_batch_size,\n",
    "    \"gradient_accumulation_steps\": tcm.gradient_accumulation_steps,\n",
    "    \"learning_rate\": tcm.learning_rate,\n",
    "    \"weight_decay\": tcm.weight_decay,\n",
    "    \"warmup_steps\": tcm.warmup_steps,\n",
    "    \"lr_scheduler_type\": tcm.lr_scheduler_type,\n",
    "    \"fp16\": tcm.fp16,\n",
    "    \"bf16\": tcm.bf16,\n",
    "    \"gradient_checkpointing\": tcm.gradient_checkpointing,\n",
    "    \"dataloader_pin_memory\": tcm.dataloader_pin_memory,\n",
    "    \"eval_strategy\": tcm.eval_strategy,\n",
    "    \"eval_steps\": tcm.eval_steps,\n",
    "    \"logging_steps\": tcm.logging_steps,\n",
    "    \"save_strategy\": tcm.save_strategy,\n",
    "    \"save_steps\": tcm.save_steps,\n",
    "    \"save_total_limit\": tcm.save_total_limit,\n",
    "    \"load_best_model_at_end\": tcm.load_best_model_at_end,\n",
    "    \"metric_for_best_model\": tcm.metric_for_best_model,\n",
    "    \"greater_is_better\": tcm.greater_is_better,\n",
    "    \"remove_unused_columns\": tcm.remove_unused_columns,\n",
    "    \"dataloader_num_workers\": tcm.dataloader_num_workers,\n",
    "    \"report_to\": \"none\",\n",
    "    \"seed\": 42,\n",
    "    **extra_optim,\n",
    "}\n",
    "\n",
    "# dÃ¹ng max_steps nhÆ° trÆ°á»›c\n",
    "sched = tcm.calculate_training_steps(dataset_size)\n",
    "max_steps = sched['total_training_steps']\n",
    "if max_steps and max_steps > 0:\n",
    "    args_dict[\"max_steps\"] = max_steps\n",
    "    args_dict.pop(\"num_train_epochs\", None)\n",
    "\n",
    "# 7) lá»c field theo version transformers hiá»‡n táº¡i\n",
    "ta_field_names = {f.name for f in dataclass_fields(HFTrainingArguments)}\n",
    "if \"eval_strategy\" not in ta_field_names and \"eval_strategy\" in ta_field_names:\n",
    "    args_dict[\"eval_strategy\"] = args_dict.pop(\"eval_strategy\", \"steps\")\n",
    "filtered_args = {k: v for k, v in args_dict.items() if k in ta_field_names}\n",
    "\n",
    "training_args = HFTrainingArguments(**filtered_args)\n",
    "\n",
    "print(\"\\n== Preview key TrainingArguments ==\")\n",
    "print(\"output_dir:\", training_args.output_dir)\n",
    "print(\"per_device_train_batch_size:\", training_args.per_device_train_batch_size)\n",
    "print(\"gradient_accumulation_steps:\", training_args.gradient_accumulation_steps)\n",
    "print(\"learning_rate:\", training_args.learning_rate)\n",
    "print(\"eval/log/save steps:\",\n",
    "      getattr(training_args, \"eval_steps\", None),\n",
    "      getattr(training_args, \"logging_steps\", None),\n",
    "      getattr(training_args, \"save_steps\", None))\n",
    "print(\"precision fp16/bf16:\", getattr(training_args, \"fp16\", None), getattr(training_args, \"bf16\", None))\n",
    "print(\"optim:\", getattr(training_args, \"optim\", None))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "f79bd45d-7e9f-495b-a9c6-efb03345503c",
    "_uuid": "958644e3-5b73-40c5-b596-a4bb2eb5d186",
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "trusted": true
   },
   "source": [
    "## Session 4 â€” Fine-tune Qwen\n",
    "Import `QwenBankingFineTuner` tá»« `fine_tune_qwen.py` (khÃ´ng cháº¡y `main()`), rá»“i cháº¡y train báº±ng Ä‘Æ°á»ng dáº«n Kaggle.\n",
    "\n",
    "> **Ghi chÃº:** Náº¿u khÃ´ng báº­t GPU, tham sá»‘ `attn_implementation='flash_attention_2'` trong code chá»‰ Ä‘Æ°á»£c set khi CUDA kháº£ dá»¥ng. Náº¿u dÃ¹ng CPU-only, báº¡n váº«n cÃ³ thá»ƒ import lá»›p vÃ  huáº¥n luyá»‡n cháº­m hÆ¡n."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T17:17:13.537664Z",
     "iopub.status.busy": "2025-09-17T17:17:13.537205Z",
     "iopub.status.idle": "2025-09-17T17:17:40.617256Z",
     "shell.execute_reply": "2025-09-17T17:17:40.615970Z",
     "shell.execute_reply.started": "2025-09-17T17:17:13.537634Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m8.4/8.4 MB\u001b[0m \u001b[31m69.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "  Building wheel for flash-attn (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!pip -q install \"flash-attn>=2.5.9\" --no-build-isolation || echo \"flash-attn install failed; will fallback to SDPA\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "dd47899c-9bb6-4906-a973-c5ff19dc5371",
    "_uuid": "4aca1324-b9bd-47c7-9518-e89a3c25356e",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-17T17:17:40.619342Z",
     "iopub.status.busy": "2025-09-17T17:17:40.618960Z",
     "iopub.status.idle": "2025-09-17T17:17:43.697191Z",
     "shell.execute_reply": "2025-09-17T17:17:43.696290Z",
     "shell.execute_reply.started": "2025-09-17T17:17:40.619298Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "Fine-tuning Script for Qwen3-4B Banking Chatbot\n",
    "Script chÃ­nh Ä‘á»ƒ fine-tune model vá»›i LoRA\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import json\n",
    "import torch\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    DataCollatorForSeq2Seq,\n",
    "    Trainer\n",
    ")\n",
    "from peft import get_peft_model, prepare_model_for_kbit_training\n",
    "from datasets import Dataset\n",
    "from typing import Dict, List, Any\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "# === LÆ¯U Ã ===\n",
    "# Trong notebook, báº¡n Ä‘Ã£ cÃ³:\n",
    "#  - LoRAConfigManager, get_recommended_config (tá»« pháº§n LoRA)\n",
    "#  - TrainingConfigManager, get_recommended_training_config (tá»« pháº§n Training)\n",
    "# Náº¿u muá»‘n import nhÆ° file .py: hÃ£y %%writefile lora_config.py & training_config.py trÆ°á»›c, rá»“i má»Ÿ 2 dÃ²ng dÆ°á»›i:\n",
    "# from lora_config import LoRAConfigManager, get_recommended_config\n",
    "# from training_config import TrainingConfigManager, get_recommended_training_config\n",
    "\n",
    "logging.basicConfig(level=logging.INFO)\n",
    "logger = logging.getLogger(__name__)\n",
    "\n",
    "class QwenBankingFineTuner:\n",
    "    def __init__(self, \n",
    "                 model_name: str = \"Qwen/Qwen3-4B\",\n",
    "                 data_dir: str = \"/kaggle/working/data/processed/train_split\",\n",
    "                 output_dir: str = \"/kaggle/working/qwen-banking-lora\"):\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.data_dir = data_dir\n",
    "        self.output_dir = output_dir\n",
    "        self.device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "        \n",
    "        # Initialize components\n",
    "        self.tokenizer = None\n",
    "        self.model = None\n",
    "        self.train_dataset = None\n",
    "        self.eval_dataset = None\n",
    "        \n",
    "        logger.info(f\"Initializing fine-tuner for {model_name}\")\n",
    "        logger.info(f\"Device: {self.device}\")\n",
    "        if torch.cuda.is_available():\n",
    "            logger.info(f\"GPU: {torch.cuda.get_device_name()}\")\n",
    "            logger.info(f\"VRAM: {torch.cuda.get_device_properties(0).total_memory / 1e9:.1f} GB\")\n",
    "    \n",
    "    def load_tokenizer_and_model(self):\n",
    "        \"\"\"Load tokenizer and model\"\"\"\n",
    "        logger.info(\"Loading tokenizer and model...\")\n",
    "        \n",
    "        # Load tokenizer\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "            self.model_name,\n",
    "            trust_remote_code=True,\n",
    "            padding_side=\"right\"\n",
    "        )\n",
    "        \n",
    "        # Add pad token if not exists\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # Load model with optimizations\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            attn_implementation=\"flash_attention_2\" if torch.cuda.is_available() else None\n",
    "        )\n",
    "        \n",
    "        # Prepare model for LoRA\n",
    "        self.model = prepare_model_for_kbit_training(self.model)\n",
    "        \n",
    "        logger.info(\"Model and tokenizer loaded successfully\")\n",
    "    \n",
    "    def load_datasets(self):\n",
    "        \"\"\"Load training and validation datasets\"\"\"\n",
    "        logger.info(\"Loading datasets...\")\n",
    "        \n",
    "        def load_jsonl(filepath: str) -> List[Dict[str, Any]]:\n",
    "            data = []\n",
    "            with open(filepath, 'r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    data.append(json.loads(line.strip()))\n",
    "            return data\n",
    "        \n",
    "        # Load data\n",
    "        train_data = load_jsonl(os.path.join(self.data_dir, \"train.jsonl\"))\n",
    "        val_data = load_jsonl(os.path.join(self.data_dir, \"validation.jsonl\"))\n",
    "        \n",
    "        # Convert to datasets\n",
    "        self.train_dataset = Dataset.from_list(train_data)\n",
    "        self.eval_dataset = Dataset.from_list(val_data)\n",
    "        \n",
    "        logger.info(f\"Loaded {len(self.train_dataset)} training samples\")\n",
    "        logger.info(f\"Loaded {len(self.eval_dataset)} validation samples\")\n",
    "    \n",
    "    def preprocess_function(self, examples):\n",
    "        \"\"\"Preprocess examples for training\"\"\"\n",
    "        model_inputs = {\"input_ids\": [], \"attention_mask\": [], \"labels\": []}\n",
    "        \n",
    "        for messages in examples[\"messages\"]:\n",
    "            # Apply chat template\n",
    "            text = self.tokenizer.apply_chat_template(\n",
    "                messages,\n",
    "                tokenize=False,\n",
    "                add_generation_prompt=False\n",
    "            )\n",
    "            \n",
    "            # Tokenize\n",
    "            tokenized = self.tokenizer(\n",
    "                text,\n",
    "                truncation=True,\n",
    "                max_length=1024,\n",
    "                padding=False,\n",
    "                return_tensors=None\n",
    "            )\n",
    "            \n",
    "            model_inputs[\"input_ids\"].append(tokenized[\"input_ids\"])\n",
    "            model_inputs[\"attention_mask\"].append(tokenized[\"attention_mask\"])\n",
    "            \n",
    "            # Labels are the same as input_ids for causal LM\n",
    "            model_inputs[\"labels\"].append(tokenized[\"input_ids\"].copy())\n",
    "        \n",
    "        return model_inputs\n",
    "    \n",
    "    def setup_lora(self, available_vram_gb: float = 12.0):\n",
    "        \"\"\"Setup LoRA configuration\"\"\"\n",
    "        logger.info(\"Setting up LoRA...\")\n",
    "        \n",
    "        # Get recommended LoRA config (Ä‘Ã£ Ä‘á»‹nh nghÄ©a á»Ÿ session LoRA)\n",
    "        lora_config = get_recommended_config(available_vram_gb)\n",
    "        \n",
    "        # Apply LoRA to model\n",
    "        self.model = get_peft_model(self.model, lora_config)\n",
    "        \n",
    "        # Print trainable parameters\n",
    "        self.model.print_trainable_parameters()\n",
    "        \n",
    "        logger.info(\"LoRA setup completed\")\n",
    "    \n",
    "    def train(self, available_vram_gb: float = 12.0):\n",
    "        \"\"\"Main training function\"\"\"\n",
    "        logger.info(\"Starting training...\")\n",
    "        \n",
    "        # Preprocess datasets\n",
    "        train_dataset = self.train_dataset.map(\n",
    "            self.preprocess_function,\n",
    "            batched=True,\n",
    "            remove_columns=self.train_dataset.column_names\n",
    "        )\n",
    "        \n",
    "        eval_dataset = self.eval_dataset.map(\n",
    "            self.preprocess_function,\n",
    "            batched=True,\n",
    "            remove_columns=self.eval_dataset.column_names\n",
    "        )\n",
    "        \n",
    "        # Get training arguments (Ä‘Ã£ Ä‘á»‹nh nghÄ©a á»Ÿ session Training)\n",
    "        training_args = get_recommended_training_config(\n",
    "            available_vram_gb, \n",
    "            len(train_dataset)\n",
    "        )\n",
    "        training_args.output_dir = self.output_dir\n",
    "        \n",
    "        # Data collator\n",
    "        data_collator = DataCollatorForSeq2Seq(\n",
    "            tokenizer=self.tokenizer,\n",
    "            model=self.model,\n",
    "            padding=True,\n",
    "            pad_to_multiple_of=8,   # thÃªm dÃ²ng nÃ y\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        # Initialize trainer\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=train_dataset,\n",
    "            eval_dataset=eval_dataset,\n",
    "            tokenizer=self.tokenizer,\n",
    "            data_collator=data_collator,\n",
    "        )\n",
    "        \n",
    "        # Start training\n",
    "        start_time = datetime.now()\n",
    "        logger.info(f\"Training started at {start_time}\")\n",
    "        \n",
    "        trainer.train()\n",
    "        \n",
    "        end_time = datetime.now()\n",
    "        training_duration = end_time - start_time\n",
    "        logger.info(f\"Training completed at {end_time}\")\n",
    "        logger.info(f\"Training duration: {training_duration}\")\n",
    "        \n",
    "        # Save the final model\n",
    "        trainer.save_model()\n",
    "        self.tokenizer.save_pretrained(self.output_dir)\n",
    "        \n",
    "        logger.info(f\"Model saved to {self.output_dir}\")\n",
    "        \n",
    "        return trainer\n",
    "    \n",
    "    def run_fine_tuning(self, available_vram_gb: float = 12.0):\n",
    "        \"\"\"Complete fine-tuning pipeline\"\"\"\n",
    "        try:\n",
    "            # Load model and tokenizer\n",
    "            self.load_tokenizer_and_model()\n",
    "            \n",
    "            # Load datasets\n",
    "            self.load_datasets()\n",
    "            \n",
    "            # Setup LoRA\n",
    "            self.setup_lora(available_vram_gb)\n",
    "            \n",
    "            # Train model\n",
    "            trainer = self.train(available_vram_gb)\n",
    "            \n",
    "            logger.info(\"Fine-tuning completed successfully!\")\n",
    "            return trainer\n",
    "            \n",
    "        except Exception as e:\n",
    "            logger.error(f\"Error during fine-tuning: {str(e)}\")\n",
    "            raise\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **QLoRA 4-bit hay fp16 fallback**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T17:17:43.699347Z",
     "iopub.status.busy": "2025-09-17T17:17:43.698275Z",
     "iopub.status.idle": "2025-09-17T17:17:43.709457Z",
     "shell.execute_reply": "2025-09-17T17:17:43.708614Z",
     "shell.execute_reply.started": "2025-09-17T17:17:43.699316Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "# Safe patch: Æ°u tiÃªn QLoRA 4-bit, náº¿u fail thá»­ 8-bit, cuá»‘i cÃ¹ng fallback fp16\n",
    "import torch, logging\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "def _safe_load_tokenizer_and_model(self):\n",
    "    logger = logging.getLogger(__name__)\n",
    "    logger.info(\"Loading tokenizer/model with auto QLoRA(4b)->8b->fp16 fallback...\")\n",
    "\n",
    "    # Tokenizer\n",
    "    self.tokenizer = AutoTokenizer.from_pretrained(\n",
    "        self.model_name, trust_remote_code=True, padding_side=\"right\"\n",
    "    )\n",
    "    if self.tokenizer.pad_token is None:\n",
    "        self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "\n",
    "    # Attention backend\n",
    "    attn_impl = None\n",
    "    if torch.cuda.is_available():\n",
    "        try:\n",
    "            import flash_attn  # noqa\n",
    "            attn_impl = \"flash_attention_2\"\n",
    "            logger.info(\"FlashAttention-2 detected.\")\n",
    "        except Exception:\n",
    "            attn_impl = \"sdpa\"\n",
    "            logger.info(\"Using SDPA attention.\")\n",
    "    self.attn_impl_chosen = attn_impl\n",
    "\n",
    "    # Thá»­ import BnB & BitsAndBytesConfig\n",
    "    use_bnb = False\n",
    "    try:\n",
    "        from transformers import BitsAndBytesConfig\n",
    "        import bitsandbytes as bnb  # noqa\n",
    "        use_bnb = True\n",
    "    except Exception as e:\n",
    "        logger.warning(f\"bitsandbytes/BitsAndBytesConfig not available -> fallback fp16. Detail: {e}\")\n",
    "\n",
    "    # 4-bit -> 8-bit -> fp16\n",
    "    if use_bnb:\n",
    "        # Æ¯u tiÃªn 4-bit\n",
    "        try:\n",
    "            bnb_cfg = BitsAndBytesConfig(\n",
    "                load_in_4bit=True,\n",
    "                bnb_4bit_quant_type=\"nf4\",\n",
    "                bnb_4bit_use_double_quant=True,\n",
    "                bnb_4bit_compute_dtype=torch.float16\n",
    "            )\n",
    "            self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                self.model_name,\n",
    "                device_map=\"auto\",\n",
    "                trust_remote_code=True,\n",
    "                quantization_config=bnb_cfg,\n",
    "                attn_implementation=attn_impl\n",
    "            )\n",
    "            self.load_mode = \"4bit\"\n",
    "            logger.info(\"Loaded model with QLoRA (4-bit).\")\n",
    "        except Exception as e4:\n",
    "            logger.warning(f\"QLoRA 4-bit failed -> try 8-bit. Detail: {e4}\")\n",
    "            # Thá»­ 8-bit\n",
    "            try:\n",
    "                bnb_cfg8 = BitsAndBytesConfig(load_in_8bit=True)\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    self.model_name,\n",
    "                    device_map=\"auto\",\n",
    "                    trust_remote_code=True,\n",
    "                    quantization_config=bnb_cfg8,\n",
    "                    attn_implementation=attn_impl\n",
    "                )\n",
    "                self.load_mode = \"8bit\"\n",
    "                logger.info(\"Loaded model with 8-bit quantization.\")\n",
    "            except Exception as e8:\n",
    "                logger.warning(f\"8-bit failed -> fallback fp16. Detail: {e8}\")\n",
    "                self.model = AutoModelForCausalLM.from_pretrained(\n",
    "                    self.model_name,\n",
    "                    torch_dtype=torch.float16,\n",
    "                    device_map=\"auto\",\n",
    "                    trust_remote_code=True,\n",
    "                    attn_implementation=attn_impl\n",
    "                )\n",
    "                self.load_mode = \"fp16\"\n",
    "                logger.info(\"Loaded model in fp16.\")\n",
    "    else:\n",
    "        # KhÃ´ng cÃ³ BnB -> fp16\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            self.model_name,\n",
    "            torch_dtype=torch.float16,\n",
    "            device_map=\"auto\",\n",
    "            trust_remote_code=True,\n",
    "            attn_implementation=attn_impl\n",
    "        )\n",
    "        self.load_mode = \"fp16\"\n",
    "        logger.info(\"Loaded model in fp16.\")\n",
    "\n",
    "    # Há»£p gradient checkpointing\n",
    "    if hasattr(self.model, \"config\") and getattr(self.model.config, \"use_cache\", False):\n",
    "        self.model.config.use_cache = False\n",
    "\n",
    "    # Chuáº©n bá»‹ cho k-bit training (OK cho 4b/8b/fp16)\n",
    "    from peft import prepare_model_for_kbit_training\n",
    "    self.model = prepare_model_for_kbit_training(self.model)\n",
    "    logger.info(f\"Model ready. load_mode={self.load_mode}, attn_impl={self.attn_impl_chosen}\")\n",
    "\n",
    "QwenBankingFineTuner.load_tokenizer_and_model = _safe_load_tokenizer_and_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T17:17:43.710726Z",
     "iopub.status.busy": "2025-09-17T17:17:43.710376Z",
     "iopub.status.idle": "2025-09-17T17:17:44.562487Z",
     "shell.execute_reply": "2025-09-17T17:17:44.561557Z",
     "shell.execute_reply.started": "2025-09-17T17:17:43.710700Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPU cache cleared.\n"
     ]
    }
   ],
   "source": [
    "import gc, torch\n",
    "for obj in [\"trainer\", \"fine_tuner\"]:\n",
    "    if obj in globals():\n",
    "        try:\n",
    "            del globals()[obj]\n",
    "        except: pass\n",
    "gc.collect()\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.empty_cache()\n",
    "    # giáº£m phÃ¢n máº£nh bá»™ nhá»›:\n",
    "    os.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n",
    "print(\"GPU cache cleared.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Cáº¥u hÃ¬nh Kaggle & cháº¡y fine-tuning (thay cho main())**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "_cell_guid": "eb259274-24b1-4361-95e7-1c9c37d2d1e4",
    "_uuid": "ad2bb489-795e-40b2-a619-2a24c9b1ea66",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-09-17T17:21:48.466550Z",
     "iopub.status.busy": "2025-09-17T17:21:48.466168Z",
     "iopub.status.idle": "2025-09-17T17:21:49.502239Z",
     "shell.execute_reply": "2025-09-17T17:21:49.501057Z",
     "shell.execute_reply.started": "2025-09-17T17:21:48.466489Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "scrolled": true,
    "trusted": true
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mImportError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_36/2561111784.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     21\u001b[0m \u001b[0mfine_tuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mquant_mode\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"4bit\"\u001b[0m   \u001b[0;31m# QLoRA 4-bit (cáº§n GPU cc >= 7.0: T4/V100/A100)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0;31m#fine_tuner.quant_mode = \"8bit\" # 8-bit (á»•n náº¿u 4-bit khÃ´ng cháº¡y hoáº·c GPU lÃ  P100)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m \u001b[0mtrainer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfine_tuner\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_fine_tuning\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mavailable_vram\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m \u001b[0;31m# (Tuá»³ chá»n) In má»™t sá»‘ thá»‘ng kÃª cuá»‘i\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_36/982986746.py\u001b[0m in \u001b[0;36mrun_fine_tuning\u001b[0;34m(self, available_vram_gb)\u001b[0m\n\u001b[1;32m    214\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    215\u001b[0m             \u001b[0;31m# Load model and tokenizer\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 216\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload_tokenizer_and_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    217\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    218\u001b[0m             \u001b[0;31m# Load datasets\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/tmp/ipykernel_36/447623975.py\u001b[0m in \u001b[0;36m_enforced_quant_loader\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     63\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m     \u001b[0;31m# 6) Load model vá»›i quant tÆ°Æ¡ng á»©ng (khÃ´ng fp16)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m     self.model = AutoModelForCausalLM.from_pretrained(\n\u001b[0m\u001b[1;32m     66\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_name\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mdevice_map\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"auto\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/models/auto/auto_factory.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m    569\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmodel_class\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig_class\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_configs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"text_config\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m                 \u001b[0mconfig\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_text_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 571\u001b[0;31m             return model_class.from_pretrained(\n\u001b[0m\u001b[1;32m    572\u001b[0m                 \u001b[0mpretrained_model_name_or_path\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0mmodel_args\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mhub_kwargs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    573\u001b[0m             )\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36m_wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    307\u001b[0m         \u001b[0mold_dtype\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    308\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 309\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    310\u001b[0m         \u001b[0;32mfinally\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    311\u001b[0m             \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_default_dtype\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mold_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/modeling_utils.py\u001b[0m in \u001b[0;36mfrom_pretrained\u001b[0;34m(cls, pretrained_model_name_or_path, config, cache_dir, ignore_mismatched_sizes, force_download, local_files_only, token, revision, use_safetensors, weights_only, *model_args, **kwargs)\u001b[0m\n\u001b[1;32m   4388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4389\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mhf_quantizer\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4390\u001b[0;31m             hf_quantizer.validate_environment(\n\u001b[0m\u001b[1;32m   4391\u001b[0m                 \u001b[0mtorch_dtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtorch_dtype\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4392\u001b[0m                 \u001b[0mfrom_tf\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfrom_tf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/transformers/quantizers/quantizer_bnb_4bit.py\u001b[0m in \u001b[0;36mvalidate_environment\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m     74\u001b[0m             )\n\u001b[1;32m     75\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mis_bitsandbytes_available\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m             raise ImportError(\n\u001b[0m\u001b[1;32m     77\u001b[0m                 \u001b[0;34m\"Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m             )\n",
      "\u001b[0;31mImportError\u001b[0m: Using `bitsandbytes` 4-bit quantization requires the latest version of bitsandbytes: `pip install -U bitsandbytes`"
     ]
    }
   ],
   "source": [
    "# Cáº¥u hÃ¬nh cho Kaggle\n",
    "MODEL_NAME = \"Qwen/Qwen3-4B\"\n",
    "DATA_DIR   = \"/kaggle/working/data/processed/train_split\"  # Ä‘Ã£ táº¡o á»Ÿ bÆ°á»›c tiá»n xá»­ lÃ½\n",
    "OUTPUT_DIR = \"/kaggle/working/qwen-banking-lora\"\n",
    "\n",
    "# PhÃ¡t hiá»‡n VRAM kháº£ dá»¥ng\n",
    "if torch.cuda.is_available():\n",
    "    available_vram = torch.cuda.get_device_properties(0).total_memory / 1e9\n",
    "    logger.info(f\"Available VRAM: {available_vram:.1f} GB\")\n",
    "else:\n",
    "    available_vram = 8.0\n",
    "    logger.warning(\"CUDA not available, using CPU\")\n",
    "\n",
    "# Khá»Ÿi táº¡o & cháº¡y fine-tuning\n",
    "fine_tuner = QwenBankingFineTuner(\n",
    "    model_name=MODEL_NAME,\n",
    "    data_dir=DATA_DIR,\n",
    "    output_dir=OUTPUT_DIR\n",
    ")\n",
    "\n",
    "trainer = fine_tuner.run_fine_tuning(available_vram)\n",
    "\n",
    "# (Tuá»³ chá»n) In má»™t sá»‘ thá»‘ng kÃª cuá»‘i\n",
    "if trainer:\n",
    "    hist = trainer.state.log_history\n",
    "    # TÃ¬m log loss cuá»‘i cÃ¹ng náº¿u cÃ³\n",
    "    final_train_loss = next((d.get('train_loss') for d in reversed(hist) if 'train_loss' in d), 'N/A')\n",
    "    final_eval_loss  = next((d.get('eval_loss') for d in reversed(hist) if 'eval_loss' in d), 'N/A')\n",
    "    print(\"\\n=== Training Statistics ===\")\n",
    "    print(\"Final training loss:\", final_train_loss)\n",
    "    print(\"Final eval loss:\", final_eval_loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-17T16:27:00.902958Z",
     "iopub.status.busy": "2025-09-17T16:27:00.902666Z",
     "iopub.status.idle": "2025-09-17T16:27:00.909705Z",
     "shell.execute_reply": "2025-09-17T16:27:00.908933Z",
     "shell.execute_reply.started": "2025-09-17T16:27:00.902934Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch: 2.6.0+cu124\n",
      "cuda available: True\n",
      "device: Tesla P100-PCIE-16GB\n",
      "capability: (6, 0)\n",
      "bitsandbytes: 0.47.0\n",
      "Linear4bit init: OK -> 4-bit likely supported\n"
     ]
    }
   ],
   "source": [
    "import torch, platform\n",
    "\n",
    "print(\"torch:\", torch.__version__)\n",
    "print(\"cuda available:\", torch.cuda.is_available())\n",
    "if torch.cuda.is_available():\n",
    "    print(\"device:\", torch.cuda.get_device_name(0))\n",
    "    print(\"capability:\", torch.cuda.get_device_capability(0))  # (major, minor)\n",
    "\n",
    "try:\n",
    "    import bitsandbytes as bnb\n",
    "    from bitsandbytes.nn import Linear4bit\n",
    "    print(\"bitsandbytes:\", bnb.__version__)\n",
    "    # Thá»­ khá»Ÿi táº¡o layer 4-bit\n",
    "    test = Linear4bit(16, 16, quant_type='nf4', compute_dtype=torch.float16)\n",
    "    print(\"Linear4bit init: OK -> 4-bit likely supported\")\n",
    "    del test\n",
    "    fourbit_ok = True\n",
    "except Exception as e:\n",
    "    print(\"4-bit check failed:\", repr(e))\n",
    "    fourbit_ok = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_quantization_status(ft):\n",
    "    print(\"load_mode:\", getattr(ft, \"load_mode\", \"<unknown>\"))\n",
    "    print(\"attn_impl:\", getattr(ft, \"attn_impl_chosen\", \"<unknown>\"))\n",
    "    qc = getattr(ft.model, \"quantization_config\", None)\n",
    "    print(\"has quantization_config:\", qc is not None)\n",
    "    if qc:\n",
    "        for k in [\"load_in_4bit\",\"bnb_4bit_quant_type\",\"bnb_4bit_use_double_quant\",\"bnb_4bit_compute_dtype\"]:\n",
    "            print(f\"  {k}:\", getattr(qc, k, None))\n",
    "    try:\n",
    "        from bitsandbytes.nn import Linear4bit\n",
    "        has_4bit = any(isinstance(m, Linear4bit) for m in ft.model.modules())\n",
    "        print(\"contains Linear4bit layers:\", has_4bit)\n",
    "    except Exception as e:\n",
    "        print(\"bitsandbytes check skipped:\", e)\n",
    "\n",
    "print_quantization_status(fine_tuner)"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 8288514,
     "sourceId": 13086163,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

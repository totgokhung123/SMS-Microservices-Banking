{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "feca4a7b",
   "metadata": {},
   "source": [
    "# Qwen3-4B + Unsloth 4-bit (QLoRA) — Banking Chatbot Fine-tune\n",
    "**Generated:** 2025-09-18 09:22:08\n",
    "\n",
    "This notebook replaces the baseline \"HF Transformers + PEFT QLoRA\" pipeline with **Unsloth 4‑bit QLoRA** for **Qwen/Qwen3-4B-Instruct-2507**.\n",
    "It is configured for **GPUs without FlashAttention2** (e.g., **Tesla P100**) and focuses on **packing**, **length‑grouping**, and **gradient accumulation** to reduce wall time.\n",
    "\n",
    "> Tip: If you previously trained with Hugging Face QLoRA and saw ~30h/epoch, Unsloth typically brings a noticeable speedup; exact gains depend on your GPU and I/O."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9556984f",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers unsloth datasets peft accelerate trl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15477943",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install bitsandbytes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea1a244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [config] Hyperparameters (carefully chosen for P100-like GPUs)\n",
    "from dataclasses import dataclass\n",
    "\n",
    "@dataclass\n",
    "class CFG:\n",
    "    base_model: str = \"Qwen/Qwen3-4B-Instruct-2507\"\n",
    "    output_dir: str = \"qwen3_4b_banking_unsloth_lora\"\n",
    "    max_seq_length: int = 1024\n",
    "    train_on_inputs: bool = False\n",
    "    lr: float = 1e-4\n",
    "    warmup_ratio: float = 0.03\n",
    "    weight_decay: float = 0.0\n",
    "    num_train_epochs: float = 2.0\n",
    "    per_device_train_batch_size: int = 1\n",
    "    gradient_accumulation_steps: int = 16\n",
    "    logging_steps: int = 10\n",
    "    eval_strategy: str = \"no\"\n",
    "    save_steps: int = 500\n",
    "    save_total_limit: int = 2\n",
    "    fp16: bool = True\n",
    "    bf16: bool = False\n",
    "    gradient_checkpointing: str = \"unsloth\"\n",
    "    lora_r: int = 16\n",
    "    lora_alpha: int = 16\n",
    "    lora_dropout: float = 0.05\n",
    "    target_modules: tuple = (\"q_proj\",\"k_proj\",\"v_proj\",\"o_proj\",\"gate_proj\",\"up_proj\",\"down_proj\")\n",
    "    use_rslora: bool = False\n",
    "    dataset_path: str = \"data/train.csv\"\n",
    "    text_columns: tuple = (\"prompt\",\"response\")\n",
    "    packing: bool = True\n",
    "    group_by_length: bool = True\n",
    "    max_steps: int | None = None\n",
    "\n",
    "CFG = CFG()\n",
    "print(CFG)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "183ef000",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [load] Model & Tokenizer via Unsloth\n",
    "import torch\n",
    "from unsloth import FastLanguageModel, is_bfloat16_supported\n",
    "from transformers import AutoTokenizer\n",
    "\n",
    "dtype = torch.float16 if not is_bfloat16_supported() else torch.bfloat16\n",
    "\n",
    "model, tokenizer = FastLanguageModel.from_pretrained(\n",
    "    model_name = CFG.base_model,\n",
    "    max_seq_length = CFG.max_seq_length,\n",
    "    load_in_4bit = True,\n",
    "    dtype = None,\n",
    "    device_map = \"auto\",\n",
    "    trust_remote_code = True,\n",
    ")\n",
    "\n",
    "model = FastLanguageModel.get_peft_model(\n",
    "    model,\n",
    "    r = CFG.lora_r,\n",
    "    lora_alpha = CFG.lora_alpha,\n",
    "    lora_dropout = CFG.lora_dropout,\n",
    "    target_modules = list(CFG.target_modules),\n",
    "    use_rslora = CFG.use_rslora,\n",
    "    bias = \"none\",\n",
    "    random_state = 42,\n",
    "    use_gradient_checkpointing = CFG.gradient_checkpointing,\n",
    ")\n",
    "\n",
    "tokenizer.padding_side = \"right\"\n",
    "tokenizer.truncation_side = \"right\"\n",
    "print(\"Loaded model with Unsloth 4-bit + LoRA.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e15dea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [data] FAST PATH: load CSV -> split -> save JSONL to /kaggle/working (no heavy cleaning)\n",
    "\n",
    "import os, random\n",
    "import pandas as pd\n",
    "from datasets import load_dataset, Dataset\n",
    "\n",
    "# ========= Config =========\n",
    "CSV_PATH     = \"/kaggle/input/data-banking-processed/final_sua_mapped_v2.csv\"  # đổi nếu khác\n",
    "OUT_DIR      = \"/kaggle/working/prepared\"\n",
    "VAL_RATIO    = 0.05\n",
    "TEST_RATIO   = 0.05\n",
    "SEED         = 42\n",
    "\n",
    "# Tùy chọn stratify theo 1 cột nhãn (nhanh nhất là để None):\n",
    "STRATIFY_COL = None  # ví dụ: \"category\" hoặc \"intent\" nếu muốn stratify\n",
    "\n",
    "# Dùng template nhanh (không gọi tokenizer.apply_chat_template để tránh vòng lặp chậm):\n",
    "USE_HF_CHAT_TEMPLATE = False  # bật True nếu bạn muốn đúng template của tokenizer (nhưng sẽ chậm hơn)\n",
    "\n",
    "SYSTEM_PROMPT = \"You are a helpful, safe, and precise banking financial assistant. Answer concisely.\"\n",
    "# ==========================\n",
    "\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "\n",
    "# 1) Load CSV (nhanh, không xử lý ngoặc nữa)\n",
    "if not os.path.exists(CSV_PATH):\n",
    "    raise FileNotFoundError(f\"CSV not found: {CSV_PATH}\")\n",
    "\n",
    "# pandas đọc nhanh; nếu muốn nữa có thể thử engine='pyarrow' nếu đã cài\n",
    "try:\n",
    "    df = pd.read_csv(CSV_PATH, dtype=str, encoding=\"utf-8\")\n",
    "except UnicodeDecodeError:\n",
    "    df = pd.read_csv(CSV_PATH, dtype=str, encoding_errors=\"ignore\")\n",
    "\n",
    "# 2) Kiểm tra 2 cột bắt buộc\n",
    "for col in [\"instruction\", \"response\"]:\n",
    "    if col not in df.columns:\n",
    "        raise ValueError(f\"❌ Thiếu cột '{col}'. Cột hiện có: {df.columns.tolist()}\")\n",
    "\n",
    "# Loại hàng rỗng/quá ngắn ở mức tối thiểu (tránh lỗi huấn luyện)\n",
    "df = df.dropna(subset=[\"instruction\", \"response\"])\n",
    "df = df[(df[\"instruction\"].str.len() >= 3) & (df[\"response\"].str.len() >= 5)].copy()\n",
    "\n",
    "# 3) Tạo text theo template nhanh (không gọi tokenizer để tránh overhead vòng lặp Python)\n",
    "def fast_template(u: str, a: str) -> str:\n",
    "    return f\"<|system|>\\n{SYSTEM_PROMPT}\\n<|user|>\\n{u}\\n<|assistant|>\\n{a}\"\n",
    "\n",
    "if USE_HF_CHAT_TEMPLATE:\n",
    "    # Cảnh báo: chậm hơn do gọi tokenizer nhiều lần\n",
    "    messages_list = [\n",
    "        [\n",
    "            {\"role\":\"system\",\"content\":SYSTEM_PROMPT},\n",
    "            {\"role\":\"user\",\"content\":str(u)},\n",
    "            {\"role\":\"assistant\",\"content\":str(a)},\n",
    "        ]\n",
    "        for u, a in zip(df[\"instruction\"].tolist(), df[\"response\"].tolist())\n",
    "    ]\n",
    "    # tokenizer phải được load trước cell này nếu bạn bật flag\n",
    "    df[\"text\"] = [tokenizer.apply_chat_template(m, tokenize=False, add_generation_prompt=False) for m in messages_list]\n",
    "else:\n",
    "    # Nhanh nhất\n",
    "    df[\"text\"] = [fast_template(u, a) for u, a in zip(df[\"instruction\"], df[\"response\"])]\n",
    "\n",
    "# 4) Split rất nhanh (random); stratify nếu bạn chỉ định cột\n",
    "random.seed(SEED)\n",
    "df = df.sample(frac=1.0, random_state=SEED).reset_index(drop=True)\n",
    "n = len(df)\n",
    "n_val  = int(round(n * VAL_RATIO))\n",
    "n_test = int(round(n * TEST_RATIO))\n",
    "n_train = max(0, n - n_val - n_test)\n",
    "\n",
    "if STRATIFY_COL and STRATIFY_COL in df.columns and df[STRATIFY_COL].nunique() >= 2:\n",
    "    # stratify đơn giản, nhưng vẫn nhẹ\n",
    "    train_idx, val_idx, test_idx = [], [], []\n",
    "    for _, g in df.groupby(STRATIFY_COL, dropna=False):\n",
    "        g = g.sample(frac=1.0, random_state=SEED)\n",
    "        gn = len(g)\n",
    "        gv = int(round(gn * VAL_RATIO))\n",
    "        gt = int(round(gn * TEST_RATIO))\n",
    "        gr = max(0, gn - gv - gt)\n",
    "        idxs = list(g.index)\n",
    "        train_idx += idxs[:gr]\n",
    "        val_idx   += idxs[gr:gr+gv]\n",
    "        test_idx  += idxs[gr+gv:gr+gv+gt]\n",
    "    # Fallback nếu lệch tổng\n",
    "    used = set(train_idx) | set(val_idx) | set(test_idx)\n",
    "    remain = [i for i in range(n) if i not in used]\n",
    "    train_idx += remain  # nhét hết phần còn lại vào train\n",
    "else:\n",
    "    train_idx = list(range(0, n_train))\n",
    "    val_idx   = list(range(n_train, n_train+n_val))\n",
    "    test_idx  = list(range(n_train+n_val, n_train+n_val+n_test))\n",
    "\n",
    "train_df = df.iloc[train_idx].copy()\n",
    "val_df   = df.iloc[val_idx].copy()\n",
    "test_df  = df.iloc[test_idx].copy()\n",
    "\n",
    "print(f\"Split sizes -> train={len(train_df)}, val={len(val_df)}, test={len(test_df)}\")\n",
    "\n",
    "# 5) Lưu JSONL ra /kaggle/working để load cực nhanh ở bước train\n",
    "train_path = os.path.join(OUT_DIR, \"train.jsonl\")\n",
    "val_path   = os.path.join(OUT_DIR, \"val.jsonl\")\n",
    "test_path  = os.path.join(OUT_DIR, \"test.jsonl\")\n",
    "\n",
    "# mỗi dòng là {\"text\": \"...\"}\n",
    "train_df[[\"text\"]].to_json(train_path, orient=\"records\", lines=True, force_ascii=False)\n",
    "val_df[[\"text\"]].to_json(val_path, orient=\"records\", lines=True, force_ascii=False)\n",
    "test_df[[\"text\"]].to_json(test_path, orient=\"records\", lines=True, force_ascii=False)\n",
    "\n",
    "print(\"Saved:\", train_path, val_path, test_path, sep=\"\\n\")\n",
    "\n",
    "# 6) (Tuỳ chọn) load lại bằng datasets để kiểm tra nhanh; SFTTrainer có thể dùng trực tiếp ds['train']\n",
    "from datasets import load_dataset\n",
    "ds = load_dataset(\"json\", data_files={\"train\": train_path, \"validation\": val_path, \"test\": test_path})\n",
    "train_ds, val_ds, test_ds = ds[\"train\"], ds[\"validation\"], ds[\"test\"]\n",
    "print(train_ds, val_ds, test_ds, sep=\"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01f258ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [train] TRL SFTTrainer with packing (padding-free) — KHÔNG dùng data_collator\n",
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "train_args = SFTConfig(\n",
    "    output_dir = CFG.output_dir,\n",
    "    num_train_epochs = CFG.num_train_epochs if CFG.max_steps is None else 1.0,\n",
    "    max_steps = CFG.max_steps if CFG.max_steps is not None else -1,\n",
    "    learning_rate = CFG.lr,\n",
    "    warmup_ratio = CFG.warmup_ratio,\n",
    "    lr_scheduler_type = \"cosine\",\n",
    "    per_device_train_batch_size = CFG.per_device_train_batch_size,\n",
    "    gradient_accumulation_steps = CFG.gradient_accumulation_steps,\n",
    "    weight_decay = CFG.weight_decay,\n",
    "    logging_steps = CFG.logging_steps,\n",
    "    save_steps = CFG.save_steps,\n",
    "    save_total_limit = CFG.save_total_limit,\n",
    "    fp16 = CFG.fp16,\n",
    "    bf16 = CFG.bf16,\n",
    "    packing = True,                 # giữ packing để tăng tốc\n",
    "    dataset_text_field = \"text\",    # dữ liệu của bạn đã là 1 field 'text'\n",
    "    dataset_num_proc = 2,\n",
    "    group_by_length = CFG.group_by_length,\n",
    "    report_to = \"none\",\n",
    "    eval_strategy = \"steps\",  # nếu muốn eval định kỳ\n",
    "    eval_steps = 500,               # tuỳ chỉnh\n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model = model,\n",
    "    tokenizer = tokenizer,\n",
    "    train_dataset = train_ds,       # dùng ds đã split/lưu JSONL\n",
    "    eval_dataset = val_ds,          # nếu bật evaluation_strategy\n",
    "    args = train_args,\n",
    "    # ❌ KHÔNG truyền data_collator khi packing=True\n",
    ")\n",
    "\n",
    "trainer.train()\n",
    "trainer.save_model(CFG.output_dir)\n",
    "tokenizer.save_pretrained(CFG.output_dir)\n",
    "print(\"Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d50b88c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# %% [export] Merge LoRA into base weights (optional) and save\n",
    "merge_dir = CFG.output_dir + \"_merged\"\n",
    "try:\n",
    "    model.save_pretrained_merged(\n",
    "        merge_dir,\n",
    "        tokenizer = tokenizer,\n",
    "        save_method = \"merged_16bit\",\n",
    "    )\n",
    "    print(\"Merged model saved to:\", merge_dir)\n",
    "except Exception as e:\n",
    "    print(\"Merge step skipped or failed:\", e)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27188cf8",
   "metadata": {},
   "source": [
    "## Notes & Rationale\n",
    "- **LoRA**: `r=16`, `alpha=16`, `dropout=0.05` balance capacity & stability for noisy mixed‑domain banking QA.  \n",
    "- **Target modules**: `q/k/v/o` + `gate/up/down` (MLP) are standard for Qwen3.  \n",
    "- **Packing & group-by-length**: reduce padding overhead on short Q/A turns.  \n",
    "- **Max sequence**: 1024 covers most banking interactions; raise if needed.  \n",
    "- **P100**: FA2 not required; speed gains come from Unsloth kernels + packing + accumulation.  \n",
    "- **Time budget**: set `CFG.max_steps` (e.g., 8000–30000) to fix wall time."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

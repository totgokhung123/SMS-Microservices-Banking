# HDBank Chatbot Fine-tuning vá»›i Qwen3-4B

Há»‡ thá»‘ng fine-tuning hoÃ n chá»‰nh cho chatbot tÆ° váº¥n tÃ i chÃ­nh ngÃ¢n hÃ ng sá»­ dá»¥ng Qwen3-4B vá»›i LoRA.

## ğŸ¯ Tá»•ng quan

Dá»± Ã¡n nÃ y fine-tune mÃ´ hÃ¬nh Qwen3-4B Ä‘á»ƒ táº¡o ra má»™t chatbot chuyÃªn biá»‡t cho tÆ° váº¥n tÃ i chÃ­nh ngÃ¢n hÃ ng. Sá»­ dá»¥ng ká»¹ thuáº­t LoRA (Low-Rank Adaptation) Ä‘á»ƒ tá»‘i Æ°u VRAM vÃ  cháº¥t lÆ°á»£ng training.

## ğŸ“‹ YÃªu cáº§u há»‡ thá»‘ng

### Pháº§n cá»©ng tá»‘i thiá»ƒu:
- **GPU**: RTX 3080 (10GB VRAM) hoáº·c tÆ°Æ¡ng Ä‘Æ°Æ¡ng
- **RAM**: 16GB
- **Storage**: 50GB trá»‘ng

### Pháº§n cá»©ng khuyáº¿n nghá»‹:
- **GPU**: RTX 4090 (24GB VRAM) hoáº·c A100
- **RAM**: 32GB+
- **Storage**: 100GB+ SSD

## ğŸš€ CÃ i Ä‘áº·t

### 1. Clone repository vÃ  cÃ i Ä‘áº·t dependencies

```bash
cd e:/HDBank_Hackathon/source
pip install -r requirements_finetune.txt
```

### 2. Chuáº©n bá»‹ dá»¯ liá»‡u

Äáº£m báº£o file CSV `final_sua_mapped_v2.csv` cÃ³ cáº¥u trÃºc:
```
tags,instruction,category,intent,response
BCIPZ,"TÃ´i muá»‘n kÃ­ch hoáº¡t tháº»...",CARD,activate_card,"TÃ´i á»Ÿ Ä‘Ã¢y Ä‘á»ƒ giÃºp..."
```

## ğŸ“Š Cáº¥u trÃºc dá»¯ liá»‡u

### Format Ä‘áº§u vÃ o (CSV):
- `instruction`: CÃ¢u há»i cá»§a khÃ¡ch hÃ ng
- `response`: CÃ¢u tráº£ lá»i cá»§a chatbot (cÃ³ thá»ƒ chá»©a placeholder)

### Format sau xá»­ lÃ½ (ChatML):
```json
{
  "messages": [
    {"role": "system", "content": "System prompt..."},
    {"role": "user", "content": "CÃ¢u há»i khÃ¡ch hÃ ng"},
    {"role": "assistant", "content": "CÃ¢u tráº£ lá»i chatbot"}
  ]
}
```

## âš™ï¸ Cáº¥u hÃ¬nh

### LoRA Configuration
```python
# Cáº¥u hÃ¬nh chuáº©n (12GB VRAM)
r=16, lora_alpha=32, dropout=0.1
target_modules=["q_proj", "k_proj", "v_proj", "o_proj", "gate_proj", "up_proj", "down_proj"]

# Cáº¥u hÃ¬nh tiáº¿t kiá»‡m memory (8GB VRAM)  
r=8, lora_alpha=16, dropout=0.1
target_modules=["q_proj", "v_proj", "down_proj"]

# Cáº¥u hÃ¬nh cháº¥t lÆ°á»£ng cao (16GB+ VRAM)
r=32, lora_alpha=64, dropout=0.05
```

### Training Configuration
```python
# Tham sá»‘ training Ä‘Æ°á»£c tá»‘i Æ°u
learning_rate=2e-4
batch_size=4 (per device)
gradient_accumulation=4 (effective batch size = 16)
epochs=3
warmup_steps=100
```

## ğŸ”§ Sá»­ dá»¥ng

### Cháº¡y pipeline hoÃ n chá»‰nh:

```bash
cd scripts
python run_complete_pipeline.py
```

### Cháº¡y tá»«ng bÆ°á»›c riÃªng láº»:

#### 1. Tiá»n xá»­ lÃ½ dá»¯ liá»‡u:
```bash
python data_preprocessing.py
```

#### 2. Fine-tuning vá»›i LoRA:
```bash
python fine_tune_qwen.py
```

#### 3. Merge model:
```bash
python merge_model.py
```

### TÃ¹y chá»n command line:

```bash
# Chá»‰ Ä‘á»‹nh file CSV khÃ¡c
python run_complete_pipeline.py --csv-path /path/to/your/data.csv

# Chá»‰ Ä‘á»‹nh thÆ° má»¥c output
python run_complete_pipeline.py --output-dir ./my-custom-model

# Sá»­ dá»¥ng model khÃ¡c
python run_complete_pipeline.py --model-name Qwen/Qwen3-7B

# Bá» qua bÆ°á»›c preprocessing
python run_complete_pipeline.py --skip-preprocessing
```

## ğŸ“ˆ Monitoring vÃ  Logging

### Logs Ä‘Æ°á»£c lÆ°u táº¡i:
- `fine_tuning_pipeline.log`: Log tá»•ng quÃ¡t
- Console output: Real-time progress

### ThÃ´ng tin Ä‘Æ°á»£c track:
- Training/validation loss
- Memory usage
- Training time
- Model parameters count
- Dataset statistics

## ğŸ›ï¸ Tá»‘i Æ°u cho VRAM khÃ¡c nhau

### 8GB VRAM (RTX 3070/4060 Ti):
```python
# Tá»± Ä‘á»™ng chá»n cáº¥u hÃ¬nh memory-efficient
batch_size=2, gradient_accumulation=8
r=8, target_modules=3
```

### 12GB VRAM (RTX 3080 Ti/4070 Ti):
```python
# Cáº¥u hÃ¬nh chuáº©n
batch_size=4, gradient_accumulation=4  
r=16, target_modules=7
```

### 16GB+ VRAM (RTX 4080/4090):
```python
# Cáº¥u hÃ¬nh cháº¥t lÆ°á»£ng cao
batch_size=8, gradient_accumulation=2
r=32, target_modules=9
```

## ğŸ“ Cáº¥u trÃºc output

Sau khi hoÃ n thÃ nh, báº¡n sáº½ cÃ³:

```
qwen-banking-merged/
â”œâ”€â”€ config.json                 # Model config
â”œâ”€â”€ model.safetensors.index.json # Model weights index
â”œâ”€â”€ model-00001-of-00002.safetensors # Model weights
â”œâ”€â”€ model-00002-of-00002.safetensors
â”œâ”€â”€ tokenizer.json              # Tokenizer
â”œâ”€â”€ tokenizer_config.json       # Tokenizer config
â”œâ”€â”€ model_info.json            # Model metadata
â””â”€â”€ inference.py               # Inference script
```

## ğŸ§ª Testing vÃ  Validation

### Test model sau khi merge:
```python
from transformers import AutoTokenizer, AutoModelForCausalLM

tokenizer = AutoTokenizer.from_pretrained("./qwen-banking-merged")
model = AutoModelForCausalLM.from_pretrained("./qwen-banking-merged")

# Test inference
messages = [
    {"role": "system", "content": "Báº¡n lÃ  trá»£ lÃ½ tÆ° váº¥n tÃ i chÃ­nh..."},
    {"role": "user", "content": "TÃ´i muá»‘n kÃ­ch hoáº¡t tháº» tÃ­n dá»¥ng"}
]

text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)
inputs = tokenizer(text, return_tensors="pt")
outputs = model.generate(**inputs, max_new_tokens=512)
response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True)
print(response)
```

## ğŸ”— TÃ­ch há»£p RAG

Model sau khi merge sáºµn sÃ ng tÃ­ch há»£p vá»›i há»‡ thá»‘ng RAG:

```python
# Sá»­ dá»¥ng merged model trong RAG pipeline
from your_rag_system import RAGPipeline

rag = RAGPipeline(
    model_path="./qwen-banking-merged",
    vector_store_path="./vector_db/",
    embedding_model="your-embedding-model"
)
```

## ğŸ› Troubleshooting

### Lá»—i CUDA Out of Memory:
```bash
# Giáº£m batch size
export CUDA_VISIBLE_DEVICES=0
python run_complete_pipeline.py  # Sáº½ tá»± Ä‘á»™ng chá»n config phÃ¹ há»£p
```

### Lá»—i Flash Attention:
```bash
# Náº¿u khÃ´ng há»— trá»£ flash attention
pip uninstall flash-attn
# Hoáº·c set attn_implementation=None trong code
```

### Lá»—i Model Loading:
```bash
# Kiá»ƒm tra HuggingFace token
huggingface-cli login
```

## ğŸ“Š Káº¿t quáº£ mong Ä‘á»£i

### Training metrics:
- **Training loss**: ~1.5-2.0 (cuá»‘i training)
- **Validation loss**: ~1.8-2.5 (tÃ¹y dataset)
- **Training time**: 2-4 giá» (RTX 4090)

### Model performance:
- **Response quality**: Cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ so vá»›i base model
- **Domain knowledge**: ChuyÃªn biá»‡t hÃ³a cho banking
- **Placeholder handling**: Xá»­ lÃ½ Ä‘Ãºng cÃ¡c placeholder

## ğŸ¤ ÄÃ³ng gÃ³p

1. Fork repository
2. Táº¡o feature branch
3. Commit changes
4. Push vÃ  táº¡o Pull Request

## ğŸ“„ License

MIT License - xem file LICENSE Ä‘á»ƒ biáº¿t thÃªm chi tiáº¿t.

## ğŸ“ Há»— trá»£

Náº¿u gáº·p váº¥n Ä‘á», vui lÃ²ng:
1. Kiá»ƒm tra logs trong `fine_tuning_pipeline.log`
2. Táº¡o issue trÃªn GitHub vá»›i thÃ´ng tin chi tiáº¿t
3. LiÃªn há»‡ team phÃ¡t triá»ƒn

---

**LÆ°u Ã½**: ÄÃ¢y lÃ  há»‡ thá»‘ng fine-tuning Ä‘Æ°á»£c tá»‘i Æ°u cho dá»± Ã¡n HDBank Hackathon. CÃ¡c tham sá»‘ Ä‘Ã£ Ä‘Æ°á»£c nghiÃªn cá»©u vÃ  test ká»¹ lÆ°á»¡ng Ä‘á»ƒ Ä‘áº¡t cháº¥t lÆ°á»£ng tá»‘t nháº¥t vá»›i tÃ i nguyÃªn cÃ³ háº¡n.